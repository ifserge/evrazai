{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a023494",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 29 18:36:57 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    36W / 300W |   1385MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       751      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    254912      C   ...nPOJSRNW-py3.8/bin/python     1377MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db96e683",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb51c258",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "510653e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chronom_test.csv   gas_test.csv     plavki_train.csv\t   sip_train.csv\r\n",
      "chronom_train.csv  gas_train.csv    produv_test.csv\t   target_train.csv\r\n",
      "chugun_test.csv    lom_test.csv     produv_train.csv\r\n",
      "chugun_train.csv   lom_train.csv    sample_submission.csv\r\n",
      "datka.zip\t   plavki_test.csv  sip_test.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2de2b175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NPLV</th>\n",
       "      <th>plavka_VR_NACH</th>\n",
       "      <th>plavka_VR_KON</th>\n",
       "      <th>plavka_NMZ</th>\n",
       "      <th>plavka_NAPR_ZAD</th>\n",
       "      <th>plavka_STFUT</th>\n",
       "      <th>plavka_TIPE_FUR</th>\n",
       "      <th>plavka_ST_FURM</th>\n",
       "      <th>plavka_TIPE_GOL</th>\n",
       "      <th>plavka_ST_GOL</th>\n",
       "      <th>TST</th>\n",
       "      <th>C</th>\n",
       "      <th>plav_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:11</td>\n",
       "      <td>2021-01-01 03:51:10</td>\n",
       "      <td>С255</td>\n",
       "      <td>МНЛЗ</td>\n",
       "      <td>971</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>11</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>11</td>\n",
       "      <td>1690</td>\n",
       "      <td>0.060</td>\n",
       "      <td>2579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510009</td>\n",
       "      <td>2021-01-01 04:00:44</td>\n",
       "      <td>2021-01-01 05:07:28</td>\n",
       "      <td>С255</td>\n",
       "      <td>МНЛЗ</td>\n",
       "      <td>972</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>12</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>12</td>\n",
       "      <td>1683</td>\n",
       "      <td>0.097</td>\n",
       "      <td>4004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>510010</td>\n",
       "      <td>2021-01-01 05:12:29</td>\n",
       "      <td>2021-01-01 06:00:53</td>\n",
       "      <td>Ст3пс/Э</td>\n",
       "      <td>Изл</td>\n",
       "      <td>973</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>13</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>13</td>\n",
       "      <td>1662</td>\n",
       "      <td>0.091</td>\n",
       "      <td>2904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>510011</td>\n",
       "      <td>2021-01-01 06:13:48</td>\n",
       "      <td>2021-01-01 07:08:39</td>\n",
       "      <td>Св-08А.z02</td>\n",
       "      <td>Изл</td>\n",
       "      <td>974</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>14</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>14</td>\n",
       "      <td>1609</td>\n",
       "      <td>0.410</td>\n",
       "      <td>3291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510012</td>\n",
       "      <td>2021-01-01 07:13:44</td>\n",
       "      <td>2021-01-01 08:01:59</td>\n",
       "      <td>SC2M/ЭТ</td>\n",
       "      <td>МНЛС</td>\n",
       "      <td>975</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>15</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>15</td>\n",
       "      <td>1682</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     NPLV      plavka_VR_NACH       plavka_VR_KON       plavka_NMZ  \\\n",
       "0  510008 2021-01-01 03:08:11 2021-01-01 03:51:10  С255              \n",
       "1  510009 2021-01-01 04:00:44 2021-01-01 05:07:28  С255              \n",
       "2  510010 2021-01-01 05:12:29 2021-01-01 06:00:53  Ст3пс/Э           \n",
       "3  510011 2021-01-01 06:13:48 2021-01-01 07:08:39  Св-08А.z02        \n",
       "4  510012 2021-01-01 07:13:44 2021-01-01 08:01:59  SC2M/ЭТ           \n",
       "\n",
       "  plavka_NAPR_ZAD  plavka_STFUT plavka_TIPE_FUR  plavka_ST_FURM  \\\n",
       "0            МНЛЗ           971  цилиндрическая              11   \n",
       "1            МНЛЗ           972  цилиндрическая              12   \n",
       "2             Изл           973  цилиндрическая              13   \n",
       "3             Изл           974  цилиндрическая              14   \n",
       "4            МНЛС           975  цилиндрическая              15   \n",
       "\n",
       "        plavka_TIPE_GOL  plavka_ST_GOL   TST      C  plav_seconds  \n",
       "0  5 сопловая                       11  1690  0.060          2579  \n",
       "1  5 сопловая                       12  1683  0.097          4004  \n",
       "2  5 сопловая                       13  1662  0.091          2904  \n",
       "3  5 сопловая                       14  1609  0.410          3291  \n",
       "4  5 сопловая                       15  1682  0.120          2895  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_static = pd.read_csv(DATA_DIR + 'plavki_train.csv').drop_duplicates('NPLV').reset_index(drop=True)\n",
    "df_test_static = pd.read_csv(DATA_DIR + 'plavki_test.csv')\n",
    "\n",
    "target = pd.read_csv(DATA_DIR + 'target_train.csv')\n",
    "df_train_static = df_train_static.merge(target, on='NPLV', how='left')\n",
    "\n",
    "for plavki in [df_train_static, df_test_static]:\n",
    "    plavki['plavka_VR_NACH'] = pd.to_datetime(plavki['plavka_VR_NACH'])\n",
    "    plavki['plavka_VR_KON'] = pd.to_datetime(plavki['plavka_VR_KON'])\n",
    "    plavki['plav_seconds'] = (((plavki.plavka_VR_KON.values - plavki.plavka_VR_NACH.values))*1e-9).astype(int)\n",
    "df_train_static.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c0029dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cat_feats = ['plavka_NAPR_ZAD', 'plavka_TIPE_FUR', 'plavka_TIPE_GOL', 'plavka_NMZ']\n",
    "static_num_feats = ['plavka_STFUT','plavka_ST_FURM','plav_seconds','plavka_ST_GOL']\n",
    "target_fts = ['TST','C']\n",
    "ts_cat_feats = ['NMSYP']\n",
    "ts_num_feats = ['V', 'T', 'O2', 'N2', 'H2', 'CO2', 'CO', 'AR',\n",
    "       'T фурмы 1', 'T фурмы 2', 'O2_pressure', 'RAS', 'POL', 'VDSYP',\n",
    "       'VSSYP', 'VES_ch', 'T_ch', 'SI_ch', 'MN_ch', 'S_ch', 'P_ch', 'CR_ch',\n",
    "       'NI_ch', 'CU_ch', 'V_ch', 'TI_ch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b69b9a9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_287456/1568610564.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  p['plav_seconds'] = int(((p.plavka_VR_KON.values[0] - p.plavka_VR_NACH.values[0])).item()*1e-9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510009\n",
      "510010\n",
      "510011\n",
      "510012\n",
      "510013\n",
      "510014\n",
      "510015\n",
      "510016\n",
      "510017\n",
      "510018\n",
      "510019\n",
      "510020\n",
      "510021\n",
      "510022\n",
      "510023\n",
      "510024\n",
      "510026\n",
      "510027\n",
      "510028\n",
      "510030\n",
      "510031\n",
      "510032\n",
      "510033\n",
      "510037\n",
      "510040\n",
      "510043\n",
      "510044\n",
      "510045\n",
      "510047\n",
      "510048\n",
      "510049\n",
      "510050\n",
      "510051\n",
      "510052\n",
      "510053\n",
      "510055\n",
      "510057\n",
      "510058\n",
      "510059\n",
      "510060\n",
      "510061\n",
      "510062\n",
      "510063\n",
      "510064\n",
      "510066\n",
      "510067\n",
      "510068\n",
      "510069\n",
      "510070\n",
      "510071\n",
      "510072\n",
      "510073\n",
      "510074\n",
      "510075\n",
      "510076\n",
      "510077\n",
      "510078\n",
      "510079\n",
      "510080\n",
      "510081\n",
      "510082\n",
      "510083\n",
      "510084\n",
      "510087\n",
      "510088\n",
      "510089\n",
      "510090\n",
      "510091\n",
      "510092\n",
      "510093\n",
      "510094\n",
      "510095\n",
      "510096\n",
      "510097\n",
      "510098\n",
      "510099\n",
      "510100\n",
      "510101\n",
      "510102\n",
      "510103\n",
      "510104\n",
      "510105\n",
      "510106\n",
      "510107\n",
      "510108\n",
      "510109\n",
      "510110\n",
      "510111\n",
      "510112\n",
      "510114\n",
      "510115\n",
      "510116\n",
      "510117\n",
      "510118\n",
      "510119\n",
      "510120\n",
      "510121\n",
      "510122\n",
      "510124\n",
      "510125\n",
      "510126\n",
      "510128\n",
      "510130\n",
      "510131\n",
      "510132\n",
      "510133\n",
      "510134\n",
      "510135\n",
      "510136\n",
      "510137\n",
      "510138\n",
      "510139\n",
      "510140\n",
      "510141\n",
      "510142\n",
      "510143\n",
      "510144\n",
      "510145\n",
      "510146\n",
      "510147\n",
      "510148\n",
      "510149\n",
      "510151\n",
      "510152\n",
      "510153\n",
      "510156\n",
      "510157\n",
      "510158\n",
      "510159\n",
      "510160\n",
      "510161\n",
      "510163\n",
      "510164\n",
      "510165\n",
      "510166\n",
      "510167\n",
      "510168\n",
      "510169\n",
      "510170\n",
      "510171\n",
      "510172\n",
      "510174\n",
      "510175\n",
      "510176\n",
      "510177\n",
      "510178\n",
      "510179\n",
      "510180\n",
      "510181\n",
      "510182\n",
      "510183\n",
      "510184\n",
      "510185\n",
      "510186\n",
      "510187\n",
      "510188\n",
      "510189\n",
      "510190\n",
      "510191\n",
      "510192\n",
      "510193\n",
      "510195\n",
      "510197\n",
      "510198\n",
      "510199\n",
      "510200\n",
      "510201\n",
      "510202\n",
      "510203\n",
      "510205\n",
      "510206\n",
      "510207\n",
      "510208\n",
      "510209\n",
      "510210\n",
      "510211\n",
      "510212\n",
      "510213\n",
      "510214\n",
      "510215\n",
      "510216\n",
      "510217\n",
      "510218\n",
      "510219\n",
      "510220\n",
      "510221\n",
      "510222\n",
      "510223\n",
      "510224\n",
      "510225\n",
      "510226\n",
      "510227\n",
      "510228\n",
      "510229\n",
      "510230\n",
      "510231\n",
      "510232\n",
      "510233\n",
      "510234\n",
      "510235\n",
      "510236\n",
      "510237\n",
      "510238\n",
      "510239\n",
      "510240\n",
      "510241\n",
      "510242\n",
      "510243\n",
      "510244\n",
      "510245\n",
      "510246\n",
      "510248\n",
      "510249\n",
      "510250\n",
      "510251\n",
      "510252\n",
      "510253\n",
      "510254\n",
      "510255\n",
      "510258\n",
      "510259\n",
      "510260\n",
      "510261\n",
      "510263\n",
      "510264\n",
      "510265\n",
      "510266\n",
      "510267\n",
      "510268\n",
      "510269\n",
      "510270\n",
      "510271\n",
      "510272\n",
      "510273\n",
      "510274\n",
      "510275\n",
      "510276\n",
      "510278\n",
      "510279\n",
      "510280\n",
      "510281\n",
      "510282\n",
      "510283\n",
      "510284\n",
      "510285\n",
      "510286\n",
      "510287\n",
      "510288\n",
      "510289\n",
      "510290\n",
      "510291\n",
      "510292\n",
      "510293\n",
      "510294\n",
      "510295\n",
      "510296\n",
      "510297\n",
      "510299\n",
      "510300\n",
      "510301\n",
      "510302\n",
      "510303\n",
      "510304\n",
      "510305\n",
      "510306\n",
      "510307\n",
      "510308\n",
      "510310\n",
      "510311\n",
      "510312\n",
      "510313\n",
      "510314\n",
      "510317\n",
      "510318\n",
      "510319\n",
      "510320\n",
      "510321\n",
      "510322\n",
      "510323\n",
      "510324\n",
      "510325\n",
      "510326\n",
      "510327\n",
      "510328\n",
      "510329\n",
      "510330\n",
      "510331\n",
      "510332\n",
      "510333\n",
      "510334\n",
      "510335\n",
      "510336\n",
      "510337\n",
      "510338\n",
      "510339\n",
      "510341\n",
      "510342\n",
      "510343\n",
      "510344\n",
      "510345\n",
      "510347\n",
      "510348\n",
      "510349\n",
      "510350\n",
      "510351\n",
      "510353\n",
      "510354\n",
      "510355\n",
      "510356\n",
      "510358\n",
      "510359\n",
      "510360\n",
      "510361\n",
      "510363\n",
      "510364\n",
      "510365\n",
      "510366\n",
      "510367\n",
      "510368\n",
      "510369\n",
      "510370\n",
      "510371\n",
      "510372\n",
      "510373\n",
      "510374\n",
      "510375\n",
      "510376\n",
      "510377\n",
      "510378\n",
      "510379\n",
      "510381\n",
      "510384\n",
      "510385\n",
      "510386\n",
      "510387\n",
      "510388\n",
      "510389\n",
      "510390\n",
      "510391\n",
      "510392\n",
      "510393\n",
      "510394\n",
      "510395\n",
      "510397\n",
      "510398\n",
      "510399\n",
      "510400\n",
      "510401\n",
      "510403\n",
      "510404\n",
      "510405\n",
      "510406\n",
      "510407\n",
      "510408\n",
      "510409\n",
      "510410\n",
      "510411\n",
      "510412\n",
      "510413\n",
      "510414\n",
      "510415\n",
      "510416\n",
      "510417\n",
      "510418\n",
      "510419\n",
      "510420\n",
      "510421\n",
      "510422\n",
      "510423\n",
      "510425\n",
      "510426\n",
      "510427\n",
      "510428\n",
      "510429\n",
      "510430\n",
      "510431\n",
      "510432\n",
      "510433\n",
      "510435\n",
      "510436\n",
      "510437\n",
      "510438\n",
      "510439\n",
      "510440\n",
      "510441\n",
      "510442\n",
      "510443\n",
      "510444\n",
      "510445\n",
      "510446\n",
      "510448\n",
      "510449\n",
      "510450\n",
      "510451\n",
      "510452\n",
      "510453\n",
      "510454\n",
      "510455\n",
      "510456\n",
      "510457\n",
      "510458\n",
      "510459\n",
      "510460\n",
      "510461\n",
      "510462\n",
      "510463\n",
      "510464\n",
      "510466\n",
      "510467\n",
      "510468\n",
      "510469\n",
      "510470\n",
      "510471\n",
      "510472\n",
      "510473\n",
      "510474\n",
      "510475\n",
      "510476\n",
      "510477\n",
      "510478\n",
      "510479\n",
      "510480\n",
      "510482\n",
      "510483\n",
      "510484\n",
      "510485\n",
      "510486\n",
      "510487\n",
      "510488\n",
      "510489\n",
      "510490\n",
      "510491\n",
      "510492\n",
      "510493\n",
      "510494\n",
      "510495\n",
      "510496\n",
      "510497\n",
      "510498\n",
      "510499\n",
      "510500\n",
      "510501\n",
      "510503\n",
      "510504\n",
      "510505\n",
      "510506\n",
      "510507\n",
      "510509\n",
      "510510\n",
      "510511\n",
      "510512\n",
      "510513\n",
      "510514\n",
      "510515\n",
      "510516\n",
      "510517\n",
      "510518\n",
      "510519\n",
      "510520\n",
      "510521\n",
      "510523\n",
      "510524\n",
      "510525\n",
      "510526\n",
      "510527\n",
      "510528\n",
      "510529\n",
      "510530\n",
      "510531\n",
      "510532\n",
      "510533\n",
      "510535\n",
      "510536\n",
      "510537\n",
      "510538\n",
      "510539\n",
      "510540\n",
      "510541\n",
      "510542\n",
      "510543\n",
      "510544\n",
      "510546\n",
      "510547\n",
      "510548\n",
      "510549\n",
      "510550\n",
      "510551\n",
      "510552\n",
      "510553\n",
      "510554\n",
      "510555\n",
      "510558\n",
      "510560\n",
      "510561\n",
      "510563\n",
      "510564\n",
      "510565\n",
      "510566\n",
      "510567\n",
      "510568\n",
      "510569\n",
      "510570\n",
      "510571\n",
      "510572\n",
      "510573\n",
      "510574\n",
      "510575\n",
      "510576\n",
      "510577\n",
      "510578\n",
      "510579\n",
      "510580\n",
      "510582\n",
      "510583\n",
      "510584\n",
      "510585\n",
      "510586\n",
      "510587\n",
      "510589\n",
      "510590\n",
      "510592\n",
      "510593\n",
      "510594\n",
      "510595\n",
      "510596\n",
      "510597\n",
      "510598\n",
      "510599\n",
      "510600\n",
      "510601\n",
      "510602\n",
      "510603\n",
      "510604\n",
      "510605\n",
      "510608\n",
      "510609\n",
      "510614\n",
      "510616\n",
      "510622\n",
      "510623\n",
      "510624\n",
      "510626\n",
      "510627\n",
      "510628\n",
      "510629\n",
      "510630\n",
      "510631\n",
      "510632\n",
      "510633\n",
      "510634\n",
      "510635\n",
      "510636\n",
      "510637\n",
      "510638\n",
      "510639\n",
      "510640\n",
      "510641\n",
      "510642\n",
      "510643\n",
      "510644\n",
      "510645\n",
      "510646\n",
      "510647\n",
      "510648\n",
      "510649\n",
      "510650\n",
      "510652\n",
      "510653\n",
      "510654\n",
      "510655\n",
      "510656\n",
      "510657\n",
      "510658\n",
      "510659\n",
      "510660\n",
      "510662\n",
      "510664\n",
      "510665\n",
      "510666\n",
      "510667\n",
      "510668\n",
      "510669\n",
      "510670\n",
      "510671\n",
      "510672\n",
      "510673\n",
      "510674\n",
      "510675\n",
      "510676\n",
      "510677\n",
      "510678\n",
      "510679\n",
      "510680\n",
      "510681\n",
      "510682\n",
      "510683\n",
      "510684\n",
      "510685\n",
      "510686\n",
      "510687\n",
      "510688\n",
      "510689\n",
      "510690\n",
      "510691\n",
      "510692\n",
      "510693\n",
      "510694\n",
      "510695\n",
      "510696\n",
      "510697\n",
      "510698\n",
      "510699\n",
      "510700\n",
      "510701\n",
      "510702\n",
      "510703\n",
      "510704\n",
      "510705\n",
      "510706\n",
      "510708\n",
      "510710\n",
      "510712\n",
      "510713\n",
      "510714\n",
      "510715\n",
      "510716\n",
      "510717\n",
      "510718\n",
      "510719\n",
      "510720\n",
      "510721\n",
      "510722\n",
      "510723\n",
      "510724\n",
      "510725\n",
      "510726\n",
      "510727\n",
      "510728\n",
      "510729\n",
      "510730\n",
      "510731\n",
      "510732\n",
      "510733\n",
      "510734\n",
      "510735\n",
      "510736\n",
      "510737\n",
      "510738\n",
      "510739\n",
      "510740\n",
      "510741\n",
      "510742\n",
      "510743\n",
      "510744\n",
      "510745\n",
      "510746\n",
      "510747\n",
      "510748\n",
      "510749\n",
      "510750\n",
      "510751\n",
      "510752\n",
      "510753\n",
      "510754\n",
      "510755\n",
      "510756\n",
      "510757\n",
      "510758\n",
      "510759\n",
      "510760\n",
      "510761\n",
      "510762\n",
      "510763\n",
      "510764\n",
      "510765\n",
      "510766\n",
      "510767\n",
      "510768\n",
      "510770\n",
      "510771\n",
      "510772\n",
      "510773\n",
      "510774\n",
      "510775\n",
      "510776\n",
      "510777\n",
      "510778\n",
      "510779\n",
      "510780\n",
      "510781\n",
      "510782\n",
      "510783\n",
      "510784\n",
      "510787\n",
      "510788\n",
      "510789\n",
      "510790\n",
      "510791\n",
      "510792\n",
      "510793\n",
      "510794\n",
      "510795\n",
      "510797\n",
      "510798\n",
      "510799\n",
      "510800\n",
      "510801\n",
      "510802\n",
      "510803\n",
      "510804\n",
      "510805\n",
      "510806\n",
      "510807\n",
      "510808\n",
      "510809\n",
      "510810\n",
      "510812\n",
      "510813\n",
      "510814\n",
      "510815\n",
      "510817\n",
      "510818\n",
      "510819\n",
      "510820\n",
      "510821\n",
      "510822\n",
      "510823\n",
      "510824\n",
      "510825\n",
      "510826\n",
      "510827\n",
      "510828\n",
      "510829\n",
      "510831\n",
      "510833\n",
      "510834\n",
      "510836\n",
      "510837\n",
      "510838\n",
      "510839\n",
      "510840\n",
      "510841\n",
      "510842\n",
      "510843\n",
      "510844\n",
      "510845\n",
      "510846\n",
      "510847\n",
      "510848\n",
      "510849\n",
      "510850\n",
      "510851\n",
      "510852\n",
      "510853\n",
      "510854\n",
      "510855\n",
      "510856\n",
      "510857\n",
      "510858\n",
      "510859\n",
      "510860\n",
      "510861\n",
      "510862\n",
      "510863\n",
      "510864\n",
      "510865\n",
      "510866\n",
      "510867\n",
      "510868\n",
      "510869\n",
      "510870\n",
      "510871\n",
      "510872\n",
      "510873\n",
      "510874\n",
      "510875\n",
      "510876\n",
      "510877\n",
      "510878\n",
      "510879\n",
      "510880\n",
      "510881\n",
      "510882\n",
      "510883\n",
      "510884\n",
      "510885\n",
      "510886\n",
      "510887\n",
      "510888\n",
      "510889\n",
      "510891\n",
      "510892\n",
      "510893\n",
      "510894\n",
      "510895\n",
      "510896\n",
      "510898\n",
      "510899\n",
      "510900\n",
      "510901\n",
      "510903\n",
      "510904\n",
      "510905\n",
      "510906\n",
      "510907\n",
      "510908\n",
      "510909\n",
      "510910\n",
      "510911\n",
      "510912\n",
      "510914\n",
      "510915\n",
      "510916\n",
      "510917\n",
      "510918\n",
      "510919\n",
      "510920\n",
      "510921\n",
      "510922\n",
      "510923\n",
      "510924\n",
      "510925\n",
      "510926\n",
      "510927\n",
      "510928\n",
      "510929\n",
      "510931\n",
      "510932\n",
      "510933\n",
      "510934\n",
      "510936\n",
      "510937\n",
      "510939\n",
      "510941\n",
      "510942\n",
      "510943\n",
      "510944\n",
      "510945\n",
      "510946\n",
      "510947\n",
      "510948\n",
      "510949\n",
      "510950\n",
      "510951\n",
      "510952\n",
      "510953\n",
      "510954\n",
      "510955\n",
      "510956\n",
      "510957\n",
      "510958\n",
      "510959\n",
      "510960\n",
      "510961\n",
      "510962\n",
      "510963\n",
      "510965\n",
      "510966\n",
      "510968\n",
      "510969\n",
      "510970\n",
      "510972\n",
      "510975\n",
      "510976\n",
      "510977\n",
      "510978\n",
      "510979\n",
      "510980\n",
      "510981\n",
      "510982\n",
      "510983\n",
      "510984\n",
      "510985\n",
      "510986\n",
      "510987\n",
      "510988\n",
      "510989\n",
      "510990\n",
      "510991\n",
      "510992\n",
      "510993\n",
      "510994\n",
      "510995\n",
      "510996\n",
      "510997\n",
      "510998\n",
      "510999\n",
      "511000\n",
      "511001\n",
      "511002\n",
      "511003\n",
      "511004\n",
      "511005\n",
      "511006\n",
      "511007\n",
      "511008\n",
      "511009\n",
      "511011\n",
      "511012\n",
      "511013\n",
      "511014\n",
      "511015\n",
      "511016\n",
      "511017\n",
      "511018\n",
      "511019\n",
      "511020\n",
      "511021\n",
      "511022\n",
      "511023\n",
      "511024\n",
      "511025\n",
      "511027\n",
      "511028\n",
      "511029\n",
      "511030\n",
      "511031\n",
      "511032\n",
      "511033\n",
      "511034\n",
      "511035\n",
      "511036\n",
      "511037\n",
      "511038\n",
      "511039\n",
      "511040\n",
      "511041\n",
      "511042\n",
      "511044\n",
      "511045\n",
      "511046\n",
      "511047\n",
      "511048\n",
      "511049\n",
      "511050\n",
      "511051\n",
      "511052\n",
      "511053\n",
      "511054\n",
      "511055\n",
      "511056\n",
      "511057\n",
      "511058\n",
      "511059\n",
      "511060\n",
      "511061\n",
      "511062\n",
      "511063\n",
      "511064\n",
      "511065\n",
      "511066\n",
      "511067\n",
      "511068\n",
      "511069\n",
      "511070\n",
      "511071\n",
      "511072\n",
      "511073\n",
      "511074\n",
      "511075\n",
      "511076\n",
      "511077\n",
      "511078\n",
      "511079\n",
      "511080\n",
      "511081\n",
      "511084\n",
      "511085\n",
      "511086\n",
      "511087\n",
      "511089\n",
      "511090\n",
      "511091\n",
      "511092\n",
      "511093\n",
      "511094\n",
      "511096\n",
      "511097\n",
      "511099\n",
      "511100\n",
      "511101\n",
      "511103\n",
      "511104\n",
      "511105\n",
      "511106\n",
      "511107\n",
      "511108\n",
      "511109\n",
      "511110\n",
      "511112\n",
      "511113\n",
      "511114\n",
      "511115\n",
      "511116\n",
      "511118\n",
      "511119\n",
      "511120\n",
      "511121\n",
      "511122\n",
      "511123\n",
      "511124\n",
      "511126\n",
      "511127\n",
      "511128\n",
      "511130\n",
      "511131\n",
      "511135\n",
      "511137\n",
      "511139\n",
      "511140\n",
      "511141\n",
      "511142\n",
      "511143\n",
      "511144\n",
      "511145\n",
      "511146\n",
      "511147\n",
      "511148\n",
      "511149\n",
      "511150\n",
      "511151\n",
      "511152\n",
      "511155\n",
      "511156\n",
      "511157\n",
      "511158\n",
      "511159\n",
      "511160\n",
      "511161\n",
      "511162\n",
      "511163\n",
      "511164\n",
      "511165\n",
      "511166\n",
      "511167\n",
      "511169\n",
      "511171\n",
      "511172\n",
      "511173\n",
      "511175\n",
      "511176\n",
      "511177\n",
      "511178\n",
      "511179\n",
      "511180\n",
      "511181\n",
      "511182\n",
      "511183\n",
      "511184\n",
      "511186\n",
      "511187\n",
      "511189\n",
      "511191\n",
      "511192\n",
      "511193\n",
      "511194\n",
      "511195\n",
      "511196\n",
      "511197\n",
      "511198\n",
      "511199\n",
      "511200\n",
      "511201\n",
      "511202\n",
      "511203\n",
      "511204\n",
      "511205\n",
      "511206\n",
      "511207\n",
      "511208\n",
      "511209\n",
      "511210\n",
      "511211\n",
      "511213\n",
      "511214\n",
      "511215\n",
      "511216\n",
      "511217\n",
      "511218\n",
      "511221\n",
      "511222\n",
      "511223\n",
      "511225\n",
      "511226\n",
      "511229\n",
      "511230\n",
      "511232\n",
      "511233\n",
      "511234\n",
      "511235\n",
      "511236\n",
      "511237\n",
      "511238\n",
      "511239\n",
      "511240\n",
      "511241\n",
      "511242\n",
      "511243\n",
      "511245\n",
      "511246\n",
      "511247\n",
      "511248\n",
      "511249\n",
      "511250\n",
      "511251\n",
      "511252\n",
      "511253\n",
      "511254\n",
      "511255\n",
      "511256\n",
      "511257\n",
      "511258\n",
      "511259\n",
      "511260\n",
      "511261\n",
      "511262\n",
      "511263\n",
      "511264\n",
      "511265\n",
      "511266\n",
      "511267\n",
      "511268\n",
      "511270\n",
      "511271\n",
      "511272\n",
      "511273\n",
      "511274\n",
      "511275\n",
      "511276\n",
      "511277\n",
      "511278\n",
      "511279\n",
      "511280\n",
      "511281\n",
      "511282\n",
      "511283\n",
      "511284\n",
      "511285\n",
      "511286\n",
      "511287\n",
      "511288\n",
      "511289\n",
      "511290\n",
      "511291\n",
      "511292\n",
      "511293\n",
      "511295\n",
      "511296\n",
      "511297\n",
      "511298\n",
      "511299\n",
      "511300\n",
      "511301\n",
      "511302\n",
      "511303\n",
      "511305\n",
      "511306\n",
      "511307\n",
      "511308\n",
      "511309\n",
      "511310\n",
      "511311\n",
      "511312\n",
      "511313\n",
      "511314\n",
      "511315\n",
      "511316\n",
      "511317\n",
      "511318\n",
      "511319\n",
      "511320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511321\n",
      "511322\n",
      "511323\n",
      "511324\n",
      "511325\n",
      "511326\n",
      "511327\n",
      "511328\n",
      "511329\n",
      "511330\n",
      "511331\n",
      "511332\n",
      "511333\n",
      "511334\n",
      "511335\n",
      "511336\n",
      "511337\n",
      "511338\n",
      "511339\n",
      "511341\n",
      "511342\n",
      "511343\n",
      "511344\n",
      "511345\n",
      "511346\n",
      "511347\n",
      "511348\n",
      "511349\n",
      "511350\n",
      "511351\n",
      "511352\n",
      "511353\n",
      "511354\n",
      "511355\n",
      "511356\n",
      "511357\n",
      "511358\n",
      "511359\n",
      "511360\n",
      "511361\n",
      "511362\n",
      "511363\n",
      "511364\n",
      "511365\n",
      "511366\n",
      "511367\n",
      "511368\n",
      "511369\n",
      "511370\n",
      "511371\n",
      "511372\n",
      "511375\n",
      "511378\n",
      "511379\n",
      "511380\n",
      "511382\n",
      "511383\n",
      "511384\n",
      "511385\n",
      "511386\n",
      "511387\n",
      "511388\n",
      "511389\n",
      "511390\n",
      "511391\n",
      "511392\n",
      "511393\n",
      "511394\n",
      "511395\n",
      "511396\n",
      "511397\n",
      "511398\n",
      "511399\n",
      "511400\n",
      "511401\n",
      "511402\n",
      "511403\n",
      "511404\n",
      "511405\n",
      "511406\n",
      "511407\n",
      "511408\n",
      "511409\n",
      "511410\n",
      "511411\n",
      "511412\n",
      "511413\n",
      "511414\n",
      "511415\n",
      "511417\n",
      "511418\n",
      "511419\n",
      "511420\n",
      "511422\n",
      "511423\n",
      "511424\n",
      "511425\n",
      "511426\n",
      "511427\n",
      "511429\n",
      "511430\n",
      "511431\n",
      "511432\n",
      "511433\n",
      "511434\n",
      "511435\n",
      "511436\n",
      "511437\n",
      "511438\n",
      "511439\n",
      "511440\n",
      "511441\n",
      "511442\n",
      "511443\n",
      "511444\n",
      "511445\n",
      "511446\n",
      "511447\n",
      "511448\n",
      "511449\n",
      "511450\n",
      "511451\n",
      "511452\n",
      "511454\n",
      "511455\n",
      "511456\n",
      "511457\n",
      "511458\n",
      "511462\n",
      "511463\n",
      "511464\n",
      "511465\n",
      "511466\n",
      "511467\n",
      "511468\n",
      "511469\n",
      "511470\n",
      "511471\n",
      "511472\n",
      "511473\n",
      "511474\n",
      "511475\n",
      "511476\n",
      "511477\n",
      "511478\n",
      "511479\n",
      "511481\n",
      "511483\n",
      "511484\n",
      "511485\n",
      "511486\n",
      "511487\n",
      "511488\n",
      "511489\n",
      "511490\n",
      "511491\n",
      "511492\n",
      "511493\n",
      "511494\n",
      "511495\n",
      "511496\n",
      "511497\n",
      "511498\n",
      "511499\n",
      "511500\n",
      "511501\n",
      "511502\n",
      "511503\n",
      "511505\n",
      "511506\n",
      "511507\n",
      "511508\n",
      "511509\n",
      "511511\n",
      "511512\n",
      "511513\n",
      "511514\n",
      "511515\n",
      "511516\n",
      "511517\n",
      "511518\n",
      "511519\n",
      "511522\n",
      "511525\n",
      "511526\n",
      "511527\n",
      "511529\n",
      "511531\n",
      "511532\n",
      "511533\n",
      "511534\n",
      "511535\n",
      "511536\n",
      "511537\n",
      "511538\n",
      "511539\n",
      "511540\n",
      "511541\n",
      "511543\n",
      "511544\n",
      "511545\n",
      "511546\n",
      "511547\n",
      "511548\n",
      "511549\n",
      "511550\n",
      "511551\n",
      "511552\n",
      "511553\n",
      "511554\n",
      "511555\n",
      "511557\n",
      "511558\n",
      "511559\n",
      "511560\n",
      "511561\n",
      "511562\n",
      "511563\n",
      "511564\n",
      "511565\n",
      "511566\n",
      "511567\n",
      "511568\n",
      "511569\n",
      "511570\n",
      "511571\n",
      "511572\n",
      "511573\n",
      "511574\n",
      "511575\n",
      "511576\n",
      "511577\n",
      "511578\n",
      "511579\n",
      "511580\n",
      "511581\n",
      "511582\n",
      "511583\n",
      "511584\n",
      "511585\n",
      "511586\n",
      "511587\n",
      "511588\n",
      "511589\n",
      "511590\n",
      "511591\n",
      "511592\n",
      "511593\n",
      "511594\n",
      "511595\n",
      "511596\n",
      "511597\n",
      "511598\n",
      "511600\n",
      "511601\n",
      "511602\n",
      "511603\n",
      "511604\n",
      "511605\n",
      "511606\n",
      "511607\n",
      "511608\n",
      "511609\n",
      "511610\n",
      "511611\n",
      "511612\n",
      "511613\n",
      "511614\n",
      "511615\n",
      "511616\n",
      "511617\n",
      "511618\n",
      "511619\n",
      "511620\n",
      "511621\n",
      "511622\n",
      "511623\n",
      "511624\n",
      "511625\n",
      "511626\n",
      "511627\n",
      "511628\n",
      "511629\n",
      "511630\n",
      "511631\n",
      "511632\n",
      "511633\n",
      "511634\n",
      "511635\n",
      "511636\n",
      "511637\n",
      "511638\n",
      "511639\n",
      "511640\n",
      "511641\n",
      "511642\n",
      "511643\n",
      "511644\n",
      "511645\n",
      "511646\n",
      "511647\n",
      "511648\n",
      "511649\n",
      "511650\n",
      "511651\n",
      "511652\n",
      "511653\n",
      "511654\n",
      "511655\n",
      "511656\n",
      "511657\n",
      "511658\n",
      "511659\n",
      "511660\n",
      "511661\n",
      "511662\n",
      "511663\n",
      "511664\n",
      "511665\n",
      "511666\n",
      "511667\n",
      "511668\n",
      "511669\n",
      "511672\n",
      "511674\n",
      "511675\n",
      "511676\n",
      "511677\n",
      "511678\n",
      "511679\n",
      "511680\n",
      "511681\n",
      "511682\n",
      "511684\n",
      "511685\n",
      "511688\n",
      "511689\n",
      "511690\n",
      "511692\n",
      "511693\n",
      "511694\n",
      "511695\n",
      "511696\n",
      "511697\n",
      "511698\n",
      "511699\n",
      "511700\n",
      "511701\n",
      "511703\n",
      "511704\n",
      "511705\n",
      "511706\n",
      "511707\n",
      "511708\n",
      "511709\n",
      "511710\n",
      "511711\n",
      "511712\n",
      "511713\n",
      "511714\n",
      "511715\n",
      "511716\n",
      "511717\n",
      "511718\n",
      "511719\n",
      "511720\n",
      "511721\n",
      "511722\n",
      "511723\n",
      "511724\n",
      "511725\n",
      "511726\n",
      "511727\n",
      "511728\n",
      "511729\n",
      "511730\n",
      "511731\n",
      "511732\n",
      "511733\n",
      "511734\n",
      "511735\n",
      "511736\n",
      "511737\n",
      "511738\n",
      "511739\n",
      "511740\n",
      "511741\n",
      "511743\n",
      "511744\n",
      "511746\n",
      "511747\n",
      "511749\n",
      "511750\n",
      "511752\n",
      "511753\n",
      "511754\n",
      "511756\n",
      "511757\n",
      "511759\n",
      "511761\n",
      "511762\n",
      "511764\n",
      "511765\n",
      "511766\n",
      "511767\n",
      "511768\n",
      "511769\n",
      "511770\n",
      "511771\n",
      "511772\n",
      "511773\n",
      "511774\n",
      "511775\n",
      "511776\n",
      "511777\n",
      "511778\n",
      "511779\n",
      "511780\n",
      "511781\n",
      "511782\n",
      "511783\n",
      "511784\n",
      "511785\n",
      "511786\n",
      "511787\n",
      "511788\n",
      "511789\n",
      "511790\n",
      "511791\n",
      "511792\n",
      "511793\n",
      "511794\n",
      "511795\n",
      "511796\n",
      "511797\n",
      "511798\n",
      "511799\n",
      "511801\n",
      "511803\n",
      "511804\n",
      "511805\n",
      "511806\n",
      "511808\n",
      "511809\n",
      "511810\n",
      "511811\n",
      "511812\n",
      "511813\n",
      "511814\n",
      "511815\n",
      "511816\n",
      "511817\n",
      "511819\n",
      "511820\n",
      "511821\n",
      "511822\n",
      "511823\n",
      "511824\n",
      "511825\n",
      "511826\n",
      "511827\n",
      "511828\n",
      "511829\n",
      "511830\n",
      "511831\n",
      "511832\n",
      "511833\n",
      "511835\n",
      "511836\n",
      "511837\n",
      "511838\n",
      "511839\n",
      "511840\n",
      "511841\n",
      "511842\n",
      "511843\n",
      "511844\n",
      "511846\n",
      "511848\n",
      "511849\n",
      "511850\n",
      "511851\n",
      "511852\n",
      "511853\n",
      "511854\n",
      "511855\n",
      "511856\n",
      "511857\n",
      "511858\n",
      "511859\n",
      "511860\n",
      "511861\n",
      "511862\n",
      "511863\n",
      "511865\n",
      "511866\n",
      "511867\n",
      "511868\n",
      "511870\n",
      "511871\n",
      "511872\n",
      "511873\n",
      "511874\n",
      "511876\n",
      "511877\n",
      "511878\n",
      "511879\n",
      "511882\n",
      "511883\n",
      "511884\n",
      "511885\n",
      "511886\n",
      "511887\n",
      "511888\n",
      "511889\n",
      "511890\n",
      "511891\n",
      "511892\n",
      "511893\n",
      "511895\n",
      "511896\n",
      "511898\n",
      "511901\n",
      "511903\n",
      "511904\n",
      "511905\n",
      "511906\n",
      "511908\n",
      "511909\n",
      "511910\n",
      "511911\n",
      "511912\n",
      "511914\n",
      "511915\n",
      "511916\n",
      "511917\n",
      "511918\n",
      "511919\n",
      "511920\n",
      "511921\n",
      "511922\n",
      "511924\n",
      "511925\n",
      "511926\n",
      "511927\n",
      "511928\n",
      "511929\n",
      "511930\n",
      "511931\n",
      "511932\n",
      "511933\n",
      "511934\n",
      "511935\n",
      "511936\n",
      "511937\n",
      "511938\n",
      "511939\n",
      "511940\n",
      "511941\n",
      "511942\n",
      "511943\n",
      "511944\n",
      "511945\n",
      "511946\n",
      "511947\n",
      "511948\n",
      "511949\n",
      "511950\n",
      "511952\n",
      "511953\n",
      "511954\n",
      "511955\n",
      "511956\n",
      "511957\n",
      "511958\n",
      "511959\n",
      "511960\n",
      "511961\n",
      "511962\n",
      "511963\n",
      "511964\n",
      "511965\n",
      "511966\n",
      "511967\n",
      "511968\n",
      "511969\n",
      "511970\n",
      "511971\n",
      "511972\n",
      "511973\n",
      "511974\n",
      "511975\n",
      "511976\n",
      "511977\n",
      "511978\n",
      "511979\n",
      "511980\n",
      "511981\n",
      "511982\n",
      "511983\n",
      "511984\n",
      "511986\n",
      "511987\n",
      "511988\n",
      "511989\n",
      "511999\n",
      "512003\n",
      "512004\n",
      "512006\n",
      "512007\n",
      "512009\n",
      "512012\n",
      "512013\n",
      "512014\n",
      "512015\n",
      "512016\n",
      "512017\n",
      "512020\n",
      "512021\n",
      "512022\n",
      "512023\n",
      "512024\n",
      "512025\n",
      "512026\n",
      "512027\n",
      "512028\n",
      "512029\n",
      "512030\n",
      "512031\n",
      "512032\n",
      "512033\n",
      "512034\n",
      "512035\n",
      "512036\n",
      "512037\n",
      "512038\n",
      "512039\n",
      "512040\n",
      "512041\n",
      "512043\n",
      "512044\n",
      "512045\n",
      "512046\n",
      "512047\n",
      "512048\n",
      "512049\n",
      "512050\n",
      "512051\n",
      "512052\n",
      "512053\n",
      "512054\n",
      "512055\n",
      "512060\n",
      "512061\n",
      "512062\n",
      "512063\n",
      "512064\n",
      "512065\n",
      "512067\n",
      "512068\n",
      "512069\n",
      "512070\n",
      "512071\n",
      "512072\n",
      "512073\n",
      "512074\n",
      "512075\n",
      "512076\n",
      "512077\n",
      "512078\n",
      "512079\n",
      "512080\n",
      "512081\n",
      "512082\n",
      "512083\n",
      "512084\n",
      "512085\n",
      "512086\n",
      "512087\n",
      "512088\n",
      "512089\n",
      "512090\n",
      "512091\n",
      "512092\n",
      "512093\n",
      "512094\n",
      "512095\n",
      "512096\n",
      "512097\n",
      "512098\n",
      "512099\n",
      "512100\n",
      "512101\n",
      "512103\n",
      "512104\n",
      "512105\n",
      "512107\n",
      "512108\n",
      "512109\n",
      "512110\n",
      "512111\n",
      "512112\n",
      "512113\n",
      "512114\n",
      "512115\n",
      "512116\n",
      "512117\n",
      "512119\n",
      "512120\n",
      "512121\n",
      "512122\n",
      "512123\n",
      "512124\n",
      "512125\n",
      "512126\n",
      "512127\n",
      "512128\n",
      "512129\n",
      "512130\n",
      "512131\n",
      "512132\n",
      "512133\n",
      "512134\n",
      "512135\n",
      "512136\n",
      "512137\n",
      "512138\n",
      "512140\n",
      "512141\n",
      "512142\n",
      "512143\n",
      "512144\n",
      "512145\n",
      "512146\n",
      "512147\n",
      "512148\n",
      "512149\n",
      "512150\n",
      "512151\n",
      "512152\n",
      "512153\n",
      "512154\n",
      "512155\n",
      "512156\n",
      "512157\n",
      "512158\n",
      "512159\n",
      "512160\n",
      "512161\n",
      "512162\n",
      "512163\n",
      "512164\n",
      "512165\n",
      "512166\n",
      "512167\n",
      "512168\n",
      "512169\n",
      "512170\n",
      "512171\n",
      "512172\n",
      "512173\n",
      "512174\n",
      "512175\n",
      "512176\n",
      "512177\n",
      "512178\n",
      "512179\n",
      "512180\n",
      "512181\n",
      "512182\n",
      "512183\n",
      "512184\n",
      "512185\n",
      "512186\n",
      "512187\n",
      "512188\n",
      "512189\n",
      "512190\n",
      "512191\n",
      "512192\n",
      "512193\n",
      "512194\n",
      "512195\n",
      "512196\n",
      "512197\n",
      "512199\n",
      "512200\n",
      "512201\n",
      "512203\n",
      "512204\n",
      "512205\n",
      "512206\n",
      "512207\n",
      "512210\n",
      "512211\n",
      "512212\n",
      "512213\n",
      "512214\n",
      "512215\n",
      "512216\n",
      "512217\n",
      "512218\n",
      "512219\n",
      "512220\n",
      "512221\n",
      "512222\n",
      "512223\n",
      "512224\n",
      "512225\n",
      "512226\n",
      "512227\n",
      "512228\n",
      "512229\n",
      "512230\n",
      "512231\n",
      "512232\n",
      "512233\n",
      "512234\n",
      "512235\n",
      "512236\n",
      "512237\n",
      "512238\n",
      "512240\n",
      "512241\n",
      "512242\n",
      "512243\n",
      "512244\n",
      "512245\n",
      "512248\n",
      "512249\n",
      "512250\n",
      "512251\n",
      "512252\n",
      "512253\n",
      "512254\n",
      "512255\n",
      "512256\n",
      "512257\n",
      "512258\n",
      "512260\n",
      "512263\n",
      "512265\n",
      "512268\n",
      "512269\n",
      "512270\n",
      "512273\n",
      "512275\n",
      "512276\n",
      "512277\n",
      "512278\n",
      "512279\n",
      "512280\n",
      "512281\n",
      "512282\n",
      "512283\n",
      "512284\n",
      "512285\n",
      "512286\n",
      "512287\n",
      "512288\n",
      "512289\n",
      "512290\n",
      "512292\n",
      "512293\n",
      "512294\n",
      "512295\n",
      "512296\n",
      "512297\n",
      "512298\n",
      "512299\n",
      "512300\n",
      "512301\n",
      "512302\n",
      "512303\n",
      "512304\n",
      "512306\n",
      "512307\n",
      "512308\n",
      "512309\n",
      "512310\n",
      "512312\n",
      "512313\n",
      "512314\n",
      "512315\n",
      "512316\n",
      "512317\n",
      "512318\n",
      "512319\n",
      "512320\n",
      "512321\n",
      "512322\n",
      "512324\n",
      "512327\n",
      "512328\n",
      "512331\n",
      "512333\n",
      "512339\n",
      "512344\n",
      "512348\n",
      "512349\n",
      "512350\n",
      "512352\n",
      "512354\n",
      "512355\n",
      "512356\n",
      "512358\n",
      "512361\n",
      "512362\n",
      "512363\n",
      "512366\n",
      "512367\n",
      "512368\n",
      "512369\n",
      "512370\n",
      "512371\n",
      "512372\n",
      "512373\n",
      "512374\n",
      "512375\n",
      "512376\n",
      "512377\n",
      "512379\n",
      "512380\n",
      "512381\n",
      "512382\n",
      "512383\n",
      "512384\n",
      "512385\n",
      "512386\n",
      "512387\n",
      "512390\n",
      "512391\n",
      "512392\n",
      "512393\n",
      "512394\n",
      "512395\n",
      "512396\n",
      "512397\n",
      "512398\n",
      "512399\n",
      "512402\n",
      "512403\n",
      "512405\n",
      "512407\n",
      "512408\n",
      "512409\n",
      "512410\n",
      "512411\n",
      "512412\n",
      "512413\n",
      "512414\n",
      "512415\n",
      "512416\n",
      "512417\n",
      "512418\n",
      "512419\n",
      "512420\n",
      "512421\n",
      "512422\n",
      "512423\n",
      "512424\n",
      "512425\n",
      "512426\n",
      "512427\n",
      "512428\n",
      "512430\n",
      "512431\n",
      "512432\n",
      "512433\n",
      "512434\n",
      "512435\n",
      "512436\n",
      "512437\n",
      "512438\n",
      "512440\n",
      "512442\n",
      "512443\n",
      "512444\n",
      "512448\n",
      "512449\n",
      "512450\n",
      "512451\n",
      "512452\n",
      "512453\n",
      "512454\n",
      "512455\n",
      "512456\n",
      "512461\n",
      "512462\n",
      "512464\n",
      "512469\n",
      "512471\n",
      "512473\n",
      "512474\n",
      "512477\n",
      "512481\n",
      "512482\n",
      "512485\n",
      "512489\n",
      "512490\n",
      "512491\n",
      "512492\n",
      "512494\n",
      "512496\n",
      "512497\n",
      "512499\n",
      "512500\n",
      "512501\n",
      "512502\n",
      "512503\n",
      "512504\n",
      "512505\n",
      "512506\n",
      "512507\n",
      "512508\n",
      "512509\n",
      "512511\n",
      "512512\n",
      "512513\n",
      "512514\n",
      "512515\n",
      "512516\n",
      "512517\n",
      "512518\n",
      "512519\n",
      "512520\n",
      "512521\n",
      "512522\n",
      "512523\n",
      "512524\n",
      "512525\n",
      "512526\n",
      "512527\n",
      "512528\n",
      "512529\n",
      "512531\n",
      "512532\n",
      "512533\n",
      "512534\n",
      "512535\n",
      "512536\n",
      "512537\n",
      "512538\n",
      "512539\n",
      "512540\n",
      "512541\n",
      "512542\n",
      "512544\n",
      "512545\n",
      "512546\n",
      "512547\n",
      "512548\n",
      "512549\n",
      "512550\n",
      "512551\n",
      "512552\n",
      "512553\n",
      "512554\n",
      "512555\n",
      "512556\n",
      "512557\n",
      "512558\n",
      "512559\n",
      "512560\n",
      "512561\n",
      "512562\n",
      "512563\n",
      "512564\n",
      "512566\n",
      "512567\n",
      "512568\n",
      "512569\n",
      "512570\n",
      "512571\n",
      "512572\n",
      "512573\n",
      "512574\n",
      "512575\n",
      "512576\n",
      "512577\n",
      "512578\n",
      "512579\n",
      "512580\n",
      "512581\n",
      "512582\n",
      "512583\n",
      "512584\n",
      "512585\n",
      "512586\n",
      "512588\n",
      "512589\n",
      "512591\n",
      "512593\n",
      "512594\n",
      "512595\n",
      "512596\n",
      "512597\n",
      "512598\n",
      "512599\n",
      "512600\n",
      "512602\n",
      "512603\n",
      "512604\n",
      "512605\n",
      "512606\n",
      "512607\n",
      "512608\n",
      "512609\n",
      "512610\n",
      "512612\n",
      "512613\n",
      "512614\n",
      "512615\n",
      "512617\n",
      "512619\n",
      "512621\n",
      "512626\n",
      "512627\n",
      "512628\n",
      "512629\n",
      "512630\n",
      "512631\n",
      "512632\n",
      "512634\n",
      "512635\n",
      "512636\n",
      "512637\n",
      "512642\n",
      "512644\n",
      "512647\n",
      "512649\n",
      "512650\n",
      "512651\n",
      "512652\n",
      "512653\n",
      "512654\n",
      "512655\n",
      "512656\n",
      "512658\n",
      "512659\n",
      "512660\n",
      "512661\n",
      "512662\n",
      "512665\n",
      "512667\n",
      "512669\n",
      "512670\n",
      "512671\n",
      "512672\n",
      "512674\n",
      "512675\n",
      "512676\n",
      "512677\n",
      "512678\n",
      "512679\n",
      "512680\n",
      "512681\n",
      "512682\n",
      "512683\n",
      "512684\n",
      "512685\n",
      "512686\n",
      "512687\n",
      "512688\n",
      "512689\n",
      "512690\n",
      "512691\n",
      "512692\n",
      "512693\n",
      "512694\n",
      "512695\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512698\n",
      "512699\n",
      "512700\n",
      "512701\n",
      "512702\n",
      "512704\n",
      "512705\n",
      "512706\n",
      "512707\n",
      "512708\n",
      "512709\n",
      "512710\n",
      "512711\n",
      "512712\n",
      "512713\n",
      "512714\n",
      "512715\n",
      "512716\n",
      "512718\n",
      "512719\n",
      "512720\n",
      "512721\n",
      "512723\n",
      "512725\n",
      "512726\n",
      "512727\n",
      "512728\n",
      "512729\n",
      "512730\n",
      "512731\n",
      "512736\n",
      "512737\n",
      "512738\n",
      "512739\n",
      "512740\n",
      "512741\n",
      "512742\n",
      "512743\n",
      "512744\n",
      "512745\n",
      "512746\n",
      "512747\n",
      "512748\n",
      "512749\n",
      "512750\n",
      "512751\n",
      "512752\n",
      "512753\n",
      "512754\n",
      "512755\n",
      "512756\n",
      "512757\n",
      "512758\n",
      "512759\n",
      "512760\n",
      "512761\n",
      "512762\n",
      "512763\n",
      "512764\n",
      "512765\n",
      "512768\n",
      "512769\n",
      "512775\n",
      "512776\n",
      "512777\n",
      "512778\n",
      "512782\n",
      "512788\n",
      "512793\n",
      "512796\n",
      "512797\n",
      "512799\n",
      "512800\n",
      "512801\n",
      "512802\n",
      "512803\n",
      "512804\n",
      "512805\n",
      "512806\n",
      "512807\n",
      "512813\n",
      "512814\n",
      "512815\n",
      "512819\n",
      "512821\n",
      "512823\n",
      "512824\n",
      "512825\n",
      "512826\n",
      "512829\n",
      "512830\n",
      "512832\n",
      "512833\n",
      "512834\n",
      "512836\n",
      "512838\n",
      "512839\n",
      "512840\n",
      "512841\n",
      "512842\n",
      "512843\n",
      "512844\n",
      "512845\n",
      "512846\n",
      "512847\n",
      "512849\n",
      "512850\n",
      "512851\n",
      "512852\n",
      "512853\n",
      "512854\n",
      "512855\n",
      "512856\n",
      "512857\n",
      "512858\n",
      "512859\n",
      "512861\n",
      "512862\n",
      "512863\n",
      "512864\n",
      "512865\n",
      "512866\n",
      "512867\n",
      "512868\n",
      "512869\n",
      "512870\n",
      "512871\n",
      "512872\n",
      "512873\n",
      "512874\n",
      "512876\n",
      "512878\n",
      "512882\n",
      "512884\n",
      "512885\n",
      "512887\n",
      "512888\n",
      "512891\n",
      "512892\n",
      "512893\n",
      "512894\n",
      "512897\n",
      "512898\n",
      "512899\n",
      "512900\n",
      "512901\n",
      "512902\n",
      "512903\n",
      "512904\n",
      "512905\n",
      "512906\n",
      "512907\n",
      "512909\n",
      "512910\n",
      "512911\n",
      "512913\n",
      "512914\n",
      "512915\n",
      "512917\n",
      "512918\n",
      "512919\n",
      "512923\n",
      "512925\n",
      "512929\n",
      "512931\n",
      "512933\n",
      "512934\n",
      "512935\n",
      "512936\n",
      "512937\n",
      "512938\n",
      "512939\n",
      "512940\n",
      "512942\n",
      "512943\n",
      "512945\n",
      "512946\n",
      "512947\n",
      "512949\n",
      "512951\n",
      "512952\n",
      "512953\n",
      "512954\n",
      "512955\n",
      "512958\n",
      "512959\n",
      "512961\n",
      "512962\n",
      "512963\n",
      "512965\n",
      "512966\n",
      "512967\n",
      "512970\n",
      "512973\n",
      "512974\n",
      "512975\n",
      "512976\n",
      "512977\n",
      "512978\n",
      "512979\n",
      "512980\n",
      "512982\n",
      "512984\n",
      "512986\n",
      "512990\n",
      "512991\n",
      "512992\n",
      "512993\n",
      "512994\n",
      "512995\n",
      "512999\n",
      "513000\n",
      "513001\n",
      "513002\n",
      "513003\n",
      "513004\n",
      "513005\n",
      "513006\n",
      "513007\n",
      "513008\n",
      "513009\n",
      "513010\n",
      "513013\n",
      "513015\n",
      "513016\n",
      "513017\n",
      "513018\n",
      "513019\n",
      "513020\n",
      "513021\n",
      "513022\n",
      "513023\n",
      "513024\n",
      "513025\n",
      "513028\n",
      "513029\n",
      "513030\n",
      "513034\n",
      "513036\n",
      "513038\n",
      "513042\n",
      "513047\n",
      "513049\n",
      "513051\n",
      "513056\n",
      "513057\n",
      "513058\n",
      "513059\n",
      "513060\n",
      "513061\n",
      "513062\n",
      "513064\n",
      "513065\n",
      "513066\n",
      "513067\n",
      "513068\n",
      "513069\n",
      "513070\n",
      "513072\n",
      "513073\n",
      "513074\n",
      "513075\n",
      "513078\n",
      "513079\n",
      "513080\n",
      "513081\n",
      "513082\n",
      "513083\n",
      "513084\n",
      "513085\n",
      "513087\n",
      "513088\n",
      "513089\n",
      "513091\n",
      "513092\n",
      "513093\n",
      "513094\n",
      "513096\n",
      "513097\n",
      "513098\n",
      "513099\n",
      "513100\n",
      "513101\n",
      "513102\n",
      "513103\n",
      "513105\n",
      "513106\n",
      "513107\n",
      "513108\n",
      "513109\n",
      "513110\n",
      "513111\n",
      "513112\n",
      "513113\n",
      "513114\n",
      "513115\n",
      "513116\n",
      "513117\n",
      "513118\n",
      "513119\n",
      "513120\n",
      "513121\n",
      "513122\n",
      "513123\n",
      "513124\n",
      "513125\n",
      "513126\n",
      "513127\n",
      "513129\n",
      "513131\n",
      "513134\n",
      "513136\n",
      "513139\n",
      "513140\n",
      "513141\n",
      "513143\n",
      "513144\n",
      "513149\n",
      "513150\n",
      "513151\n",
      "513153\n",
      "513154\n",
      "513155\n",
      "513156\n",
      "513158\n",
      "513160\n",
      "513161\n",
      "513162\n",
      "513163\n",
      "513164\n",
      "513165\n",
      "513166\n",
      "513167\n",
      "513168\n",
      "513169\n",
      "513172\n",
      "513173\n",
      "513177\n",
      "513178\n",
      "513179\n",
      "513181\n",
      "513184\n",
      "513185\n",
      "513186\n",
      "513187\n",
      "513188\n",
      "513189\n",
      "513190\n",
      "513191\n",
      "513192\n",
      "513193\n",
      "513194\n",
      "513195\n",
      "513196\n",
      "513197\n",
      "513198\n",
      "513199\n",
      "513200\n",
      "513203\n",
      "513205\n",
      "513206\n",
      "513207\n",
      "513209\n",
      "513210\n",
      "513211\n",
      "513212\n",
      "513213\n",
      "513214\n",
      "513215\n",
      "513217\n",
      "513218\n",
      "513221\n",
      "513222\n",
      "513224\n",
      "513225\n",
      "513226\n",
      "513227\n",
      "513228\n",
      "513229\n",
      "513230\n",
      "513231\n",
      "513232\n",
      "513233\n",
      "513234\n",
      "513235\n",
      "513236\n",
      "513237\n",
      "513238\n",
      "513239\n",
      "513240\n",
      "513241\n",
      "513242\n",
      "513243\n",
      "513244\n",
      "513245\n",
      "513246\n",
      "513247\n",
      "513249\n",
      "513250\n",
      "513251\n",
      "513252\n",
      "513253\n",
      "513254\n",
      "513255\n",
      "513256\n",
      "513257\n",
      "513259\n",
      "513260\n",
      "513261\n",
      "513262\n",
      "513263\n",
      "513264\n",
      "513265\n",
      "513266\n",
      "513267\n",
      "513268\n",
      "513269\n",
      "513270\n",
      "513271\n",
      "513272\n",
      "513273\n",
      "513274\n",
      "513275\n",
      "513278\n",
      "513281\n",
      "513282\n",
      "513283\n",
      "513284\n",
      "513285\n",
      "513286\n",
      "513287\n",
      "513288\n",
      "513289\n",
      "513290\n",
      "513292\n",
      "513293\n",
      "513294\n",
      "513297\n",
      "513299\n",
      "513301\n",
      "513302\n",
      "513303\n",
      "513304\n",
      "513305\n",
      "513307\n",
      "513309\n",
      "513310\n",
      "513311\n",
      "513313\n",
      "513316\n",
      "513317\n",
      "513318\n",
      "513319\n",
      "513320\n",
      "513321\n",
      "513322\n",
      "513323\n",
      "513324\n",
      "513325\n",
      "513326\n",
      "513327\n",
      "513328\n",
      "513330\n",
      "513331\n",
      "513332\n",
      "513333\n",
      "513334\n",
      "513335\n",
      "513336\n",
      "513337\n",
      "513338\n",
      "513339\n",
      "513341\n",
      "513342\n",
      "513344\n",
      "513345\n",
      "513346\n",
      "513347\n",
      "513348\n",
      "513349\n",
      "513350\n",
      "513351\n",
      "513352\n",
      "513354\n",
      "513357\n",
      "513358\n",
      "513359\n",
      "513360\n",
      "513361\n",
      "513362\n",
      "513363\n",
      "513364\n",
      "513365\n",
      "513366\n",
      "513367\n",
      "513369\n",
      "513370\n",
      "513371\n",
      "513372\n",
      "513374\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(data_dir, postfix = 'train'):\n",
    "    plavki = pd.read_csv(data_dir + 'plavki_'+postfix + '.csv')\n",
    "    plavki['plavka_VR_NACH'] = pd.to_datetime(plavki['plavka_VR_NACH'])\n",
    "    plavki['plavka_VR_KON'] = pd.to_datetime(plavki['plavka_VR_KON'])\n",
    "    gas = pd.read_csv(data_dir + 'gas_'+postfix + '.csv')\n",
    "    gas['Time'] = pd.to_datetime(gas['Time'])\n",
    "    produv = pd.read_csv(data_dir + 'produv_'+postfix + '.csv')\n",
    "    produv['SEC'] = pd.to_datetime(produv['SEC'])\n",
    "    sip = pd.read_csv(data_dir + 'sip_'+postfix + '.csv')\n",
    "    sip['DAT_OTD'] = pd.to_datetime(sip['DAT_OTD'])\n",
    "    chugun = pd.read_csv(data_dir + 'chugun_'+postfix + '.csv')\n",
    "    chugun.columns = ['NPLV'] + [x+'_ch' for x in chugun.columns.tolist()[1:-1]] + ['DATA_ZAMERA']\n",
    "    \n",
    "    dfs = []\n",
    "    for g,df in gas.groupby('NPLV'):\n",
    "        print(g)\n",
    "        p = plavki.loc[plavki.NPLV == g]\n",
    "        p['plav_seconds'] = int(((p.plavka_VR_KON.values[0] - p.plavka_VR_NACH.values[0])).item()*1e-9)\n",
    "        df1 = df.loc[df.Time <= p.plavka_VR_KON.values[0]]\n",
    "        ppdf = produv.loc[(produv.NPLV == g) & (produv.SEC <= p.plavka_VR_KON.values[0])].drop(columns=['NPLV'])\n",
    "        df1 = pd.merge_asof(df1, ppdf, left_on='Time', right_on='SEC').drop(columns=['SEC'])\n",
    "        psip = sip.loc[(produv.NPLV == g) & (sip.DAT_OTD <= p.plavka_VR_KON.values[0])].drop(columns=['NPLV'])\n",
    "        df1 = pd.merge_asof(df1, psip, left_on='Time', right_on='DAT_OTD').drop(columns=['DAT_OTD'])\n",
    "        df1 = df1.merge(chugun, on='NPLV', how='left')\n",
    "        df1 = df1.drop(columns = ['DATA_ZAMERA'])\n",
    "        dfs.append(df1)\n",
    "        #if len(dfs) > 100:\n",
    "        #    break\n",
    "        \n",
    "    df = pd.concat(dfs, ignore_index = True).reset_index(drop=True)\n",
    "    df[ts_num_feats] = df[ts_num_feats].interpolate()\n",
    "    return df\n",
    "\n",
    "df_train = make_dataset(DATA_DIR, 'train')\n",
    "df_test = make_dataset(DATA_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6646da56",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('df_train.csv', index=False)\n",
    "df_test.to_csv('df_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7a5ec22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train_static['plavka_NAPR_ZAD'])\n",
    "df_train_static['plavka_NAPR_ZAD'] = le.transform(df_train_static['plavka_NAPR_ZAD'])\n",
    "df_test_static['plavka_NAPR_ZAD'] = le.transform(df_test_static['plavka_NAPR_ZAD'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train_static['plavka_TIPE_FUR'])\n",
    "df_train_static['plavka_TIPE_FUR'] = le.transform(df_train_static['plavka_TIPE_FUR'])\n",
    "df_test_static['plavka_TIPE_FUR'] = le.transform(df_test_static['plavka_TIPE_FUR'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "plavka_nmz = set(df_train_static['plavka_NMZ'].unique().tolist()) & set(df_test_static['plavka_NMZ'].unique().tolist())\n",
    "df_train_static.loc[~df_train_static['plavka_NMZ'].isin(plavka_nmz), 'plavka_NMZ'] = 'None'\n",
    "df_test_static.loc[~df_test_static['plavka_NMZ'].isin(plavka_nmz), 'plavka_NMZ'] = 'None'\n",
    "le.fit(df_train_static['plavka_NMZ'])\n",
    "df_train_static['plavka_NMZ'] = le.transform(df_train_static['plavka_NMZ'])\n",
    "df_test_static['plavka_NMZ'] = le.transform(df_test_static['plavka_NMZ'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "plavka_nmz = set(df_train_static['plavka_TIPE_GOL'].unique().tolist()) & set(df_test_static['plavka_TIPE_GOL'].unique().tolist())\n",
    "df_train_static.loc[~df_train_static['plavka_TIPE_GOL'].isin(plavka_nmz), 'plavka_TIPE_GOL'] = 'None'\n",
    "df_test_static.loc[~df_test_static['plavka_TIPE_GOL'].isin(plavka_nmz), 'plavka_TIPE_GOL'] = 'None'\n",
    "le.fit(df_train_static['plavka_TIPE_GOL'])\n",
    "df_train_static['plavka_TIPE_GOL'] = le.transform(df_train_static['plavka_TIPE_GOL'])\n",
    "df_test_static['plavka_TIPE_GOL'] = le.transform(df_test_static['plavka_TIPE_GOL'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(df_train_static[static_num_feats])\n",
    "df_train_static[static_num_feats] = ss.transform(df_train_static[static_num_feats])\n",
    "df_test_static[static_num_feats] = ss.transform(df_test_static[static_num_feats])\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "df_train_static[target_fts] = target_scaler.fit_transform(df_train_static[target_fts])\n",
    "\n",
    "le = LabelEncoder()\n",
    "plavka_nmz = set(df_train['NMSYP'].unique().tolist()) & set(df_test['NMSYP'].unique().tolist())\n",
    "df_train.loc[~df_train['NMSYP'].isin(plavka_nmz), 'NMSYP'] = 'None'\n",
    "df_test.loc[~df_test['NMSYP'].isin(plavka_nmz), 'NMSYP'] = 'None'\n",
    "le.fit(df_train['NMSYP'])\n",
    "df_train['NMSYP'] = le.transform(df_train['NMSYP'])\n",
    "df_test['NMSYP'] = le.transform(df_test['NMSYP'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(df_train[ts_num_feats])\n",
    "df_train[ts_num_feats] = ss.transform(df_train[ts_num_feats])\n",
    "df_test[ts_num_feats] = ss.transform(df_test[ts_num_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2386d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NPLV</th>\n",
       "      <th>Time</th>\n",
       "      <th>V</th>\n",
       "      <th>T</th>\n",
       "      <th>O2</th>\n",
       "      <th>N2</th>\n",
       "      <th>H2</th>\n",
       "      <th>CO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>AR</th>\n",
       "      <th>...</th>\n",
       "      <th>T_ch</th>\n",
       "      <th>SI_ch</th>\n",
       "      <th>MN_ch</th>\n",
       "      <th>S_ch</th>\n",
       "      <th>P_ch</th>\n",
       "      <th>CR_ch</th>\n",
       "      <th>NI_ch</th>\n",
       "      <th>CU_ch</th>\n",
       "      <th>V_ch</th>\n",
       "      <th>TI_ch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:11.437</td>\n",
       "      <td>0.737329</td>\n",
       "      <td>-0.530477</td>\n",
       "      <td>0.944704</td>\n",
       "      <td>0.840958</td>\n",
       "      <td>-0.437176</td>\n",
       "      <td>-1.321086</td>\n",
       "      <td>-0.718117</td>\n",
       "      <td>0.384588</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.184487</td>\n",
       "      <td>-0.343679</td>\n",
       "      <td>1.172270</td>\n",
       "      <td>0.499569</td>\n",
       "      <td>1.421682</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>1.151996</td>\n",
       "      <td>0.290510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:12.437</td>\n",
       "      <td>0.737329</td>\n",
       "      <td>-0.530477</td>\n",
       "      <td>0.945712</td>\n",
       "      <td>0.841236</td>\n",
       "      <td>-0.436921</td>\n",
       "      <td>-1.322805</td>\n",
       "      <td>-0.718154</td>\n",
       "      <td>0.383246</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.184487</td>\n",
       "      <td>-0.343679</td>\n",
       "      <td>1.172270</td>\n",
       "      <td>0.499569</td>\n",
       "      <td>1.421682</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>1.151996</td>\n",
       "      <td>0.290510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:13.437</td>\n",
       "      <td>0.747323</td>\n",
       "      <td>-0.532999</td>\n",
       "      <td>0.946720</td>\n",
       "      <td>0.841513</td>\n",
       "      <td>-0.436666</td>\n",
       "      <td>-1.324525</td>\n",
       "      <td>-0.718192</td>\n",
       "      <td>0.381905</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.184487</td>\n",
       "      <td>-0.343679</td>\n",
       "      <td>1.172270</td>\n",
       "      <td>0.499569</td>\n",
       "      <td>1.421682</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>1.151996</td>\n",
       "      <td>0.290510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:14.437</td>\n",
       "      <td>0.757315</td>\n",
       "      <td>-0.534261</td>\n",
       "      <td>0.947728</td>\n",
       "      <td>0.841791</td>\n",
       "      <td>-0.436411</td>\n",
       "      <td>-1.326244</td>\n",
       "      <td>-0.718229</td>\n",
       "      <td>0.380564</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.184487</td>\n",
       "      <td>-0.343679</td>\n",
       "      <td>1.172270</td>\n",
       "      <td>0.499569</td>\n",
       "      <td>1.421682</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>1.151996</td>\n",
       "      <td>0.290510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:15.437</td>\n",
       "      <td>0.747323</td>\n",
       "      <td>-0.538044</td>\n",
       "      <td>0.948736</td>\n",
       "      <td>0.842068</td>\n",
       "      <td>-0.436156</td>\n",
       "      <td>-1.327964</td>\n",
       "      <td>-0.718267</td>\n",
       "      <td>0.379222</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018188</td>\n",
       "      <td>-0.184487</td>\n",
       "      <td>-0.343679</td>\n",
       "      <td>1.172270</td>\n",
       "      <td>0.499569</td>\n",
       "      <td>1.421682</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>1.151996</td>\n",
       "      <td>0.290510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466980</th>\n",
       "      <td>512322</td>\n",
       "      <td>2021-04-26 18:48:35.437</td>\n",
       "      <td>-0.287989</td>\n",
       "      <td>-1.055164</td>\n",
       "      <td>-0.799696</td>\n",
       "      <td>1.611167</td>\n",
       "      <td>-0.434369</td>\n",
       "      <td>-1.334230</td>\n",
       "      <td>-0.718529</td>\n",
       "      <td>-0.267209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.110595</td>\n",
       "      <td>0.630001</td>\n",
       "      <td>0.597517</td>\n",
       "      <td>0.772881</td>\n",
       "      <td>0.147417</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>0.209882</td>\n",
       "      <td>-0.208852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466981</th>\n",
       "      <td>512322</td>\n",
       "      <td>2021-04-26 18:48:36.437</td>\n",
       "      <td>-0.319581</td>\n",
       "      <td>-1.055164</td>\n",
       "      <td>-0.714720</td>\n",
       "      <td>1.574657</td>\n",
       "      <td>-0.446873</td>\n",
       "      <td>-1.335384</td>\n",
       "      <td>-0.718529</td>\n",
       "      <td>-0.176203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.110595</td>\n",
       "      <td>0.630001</td>\n",
       "      <td>0.597517</td>\n",
       "      <td>0.772881</td>\n",
       "      <td>0.147417</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>0.209882</td>\n",
       "      <td>-0.208852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466982</th>\n",
       "      <td>512322</td>\n",
       "      <td>2021-04-26 18:48:37.437</td>\n",
       "      <td>-0.309045</td>\n",
       "      <td>-1.055164</td>\n",
       "      <td>-0.604872</td>\n",
       "      <td>1.526879</td>\n",
       "      <td>-0.434369</td>\n",
       "      <td>-1.336538</td>\n",
       "      <td>-0.718529</td>\n",
       "      <td>-0.176203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.110595</td>\n",
       "      <td>0.630001</td>\n",
       "      <td>0.597517</td>\n",
       "      <td>0.772881</td>\n",
       "      <td>0.147417</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>0.209882</td>\n",
       "      <td>-0.208852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466983</th>\n",
       "      <td>512322</td>\n",
       "      <td>2021-04-26 18:48:38.437</td>\n",
       "      <td>-0.340669</td>\n",
       "      <td>-1.053903</td>\n",
       "      <td>-0.643215</td>\n",
       "      <td>1.543556</td>\n",
       "      <td>-0.434369</td>\n",
       "      <td>-1.335384</td>\n",
       "      <td>-0.718529</td>\n",
       "      <td>-0.267209</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.110595</td>\n",
       "      <td>0.630001</td>\n",
       "      <td>0.597517</td>\n",
       "      <td>0.772881</td>\n",
       "      <td>0.147417</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>0.209882</td>\n",
       "      <td>-0.208852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6466984</th>\n",
       "      <td>512322</td>\n",
       "      <td>2021-04-26 18:48:39.437</td>\n",
       "      <td>-0.414665</td>\n",
       "      <td>-1.053903</td>\n",
       "      <td>-0.752026</td>\n",
       "      <td>1.590433</td>\n",
       "      <td>-0.434369</td>\n",
       "      <td>-1.336538</td>\n",
       "      <td>-0.718529</td>\n",
       "      <td>-0.176203</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273195</td>\n",
       "      <td>0.110595</td>\n",
       "      <td>0.630001</td>\n",
       "      <td>0.597517</td>\n",
       "      <td>0.772881</td>\n",
       "      <td>0.147417</td>\n",
       "      <td>0.289499</td>\n",
       "      <td>0.921668</td>\n",
       "      <td>0.209882</td>\n",
       "      <td>-0.208852</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6466985 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           NPLV                    Time         V         T        O2  \\\n",
       "0        510008 2021-01-01 03:08:11.437  0.737329 -0.530477  0.944704   \n",
       "1        510008 2021-01-01 03:08:12.437  0.737329 -0.530477  0.945712   \n",
       "2        510008 2021-01-01 03:08:13.437  0.747323 -0.532999  0.946720   \n",
       "3        510008 2021-01-01 03:08:14.437  0.757315 -0.534261  0.947728   \n",
       "4        510008 2021-01-01 03:08:15.437  0.747323 -0.538044  0.948736   \n",
       "...         ...                     ...       ...       ...       ...   \n",
       "6466980  512322 2021-04-26 18:48:35.437 -0.287989 -1.055164 -0.799696   \n",
       "6466981  512322 2021-04-26 18:48:36.437 -0.319581 -1.055164 -0.714720   \n",
       "6466982  512322 2021-04-26 18:48:37.437 -0.309045 -1.055164 -0.604872   \n",
       "6466983  512322 2021-04-26 18:48:38.437 -0.340669 -1.053903 -0.643215   \n",
       "6466984  512322 2021-04-26 18:48:39.437 -0.414665 -1.053903 -0.752026   \n",
       "\n",
       "               N2        H2       CO2        CO        AR  ...      T_ch  \\\n",
       "0        0.840958 -0.437176 -1.321086 -0.718117  0.384588  ... -0.018188   \n",
       "1        0.841236 -0.436921 -1.322805 -0.718154  0.383246  ... -0.018188   \n",
       "2        0.841513 -0.436666 -1.324525 -0.718192  0.381905  ... -0.018188   \n",
       "3        0.841791 -0.436411 -1.326244 -0.718229  0.380564  ... -0.018188   \n",
       "4        0.842068 -0.436156 -1.327964 -0.718267  0.379222  ... -0.018188   \n",
       "...           ...       ...       ...       ...       ...  ...       ...   \n",
       "6466980  1.611167 -0.434369 -1.334230 -0.718529 -0.267209  ...  0.273195   \n",
       "6466981  1.574657 -0.446873 -1.335384 -0.718529 -0.176203  ...  0.273195   \n",
       "6466982  1.526879 -0.434369 -1.336538 -0.718529 -0.176203  ...  0.273195   \n",
       "6466983  1.543556 -0.434369 -1.335384 -0.718529 -0.267209  ...  0.273195   \n",
       "6466984  1.590433 -0.434369 -1.336538 -0.718529 -0.176203  ...  0.273195   \n",
       "\n",
       "            SI_ch     MN_ch      S_ch      P_ch     CR_ch     NI_ch     CU_ch  \\\n",
       "0       -0.184487 -0.343679  1.172270  0.499569  1.421682  0.289499  0.921668   \n",
       "1       -0.184487 -0.343679  1.172270  0.499569  1.421682  0.289499  0.921668   \n",
       "2       -0.184487 -0.343679  1.172270  0.499569  1.421682  0.289499  0.921668   \n",
       "3       -0.184487 -0.343679  1.172270  0.499569  1.421682  0.289499  0.921668   \n",
       "4       -0.184487 -0.343679  1.172270  0.499569  1.421682  0.289499  0.921668   \n",
       "...           ...       ...       ...       ...       ...       ...       ...   \n",
       "6466980  0.110595  0.630001  0.597517  0.772881  0.147417  0.289499  0.921668   \n",
       "6466981  0.110595  0.630001  0.597517  0.772881  0.147417  0.289499  0.921668   \n",
       "6466982  0.110595  0.630001  0.597517  0.772881  0.147417  0.289499  0.921668   \n",
       "6466983  0.110595  0.630001  0.597517  0.772881  0.147417  0.289499  0.921668   \n",
       "6466984  0.110595  0.630001  0.597517  0.772881  0.147417  0.289499  0.921668   \n",
       "\n",
       "             V_ch     TI_ch  \n",
       "0        1.151996  0.290510  \n",
       "1        1.151996  0.290510  \n",
       "2        1.151996  0.290510  \n",
       "3        1.151996  0.290510  \n",
       "4        1.151996  0.290510  \n",
       "...           ...       ...  \n",
       "6466980  0.209882 -0.208852  \n",
       "6466981  0.209882 -0.208852  \n",
       "6466982  0.209882 -0.208852  \n",
       "6466983  0.209882 -0.208852  \n",
       "6466984  0.209882 -0.208852  \n",
       "\n",
       "[6466985 rows x 29 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5e80b48d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class EvrazDatasetV(Dataset):\n",
    "    def __init__(self, feat_df, static_train_df):\n",
    "        self.feat_df = feat_df.copy()\n",
    "        self.static_train_df = static_train_df.copy()\n",
    "        \n",
    "        self.nlmv =  self.feat_df.NPLV.unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nlmv)\n",
    "\n",
    "    def __getitem__(self, idx):    \n",
    "        nlmv = self.nlmv[idx]\n",
    "        return {\"static_cat\": self.static_train_df.loc[self.static_train_df.NPLV == nlmv, static_cat_feats].fillna(0).values[0],\n",
    "                \"static_numeric\": self.static_train_df.loc[self.static_train_df.NPLV == nlmv, static_num_feats].fillna(0).values[0],\n",
    "                \"ts_cat\": self.feat_df.loc[self.feat_df.NPLV == nlmv, ts_cat_feats].fillna(0).values,\n",
    "                \"ts_numeric\": self.feat_df.loc[self.feat_df.NPLV == nlmv, ts_num_feats].fillna(0).values,\n",
    "                \"y\": self.static_train_df.loc[self.static_train_df.NPLV == nlmv, target_fts].values[0]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "06849512",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, return_attention: bool = False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.return_attention = return_attention\n",
    "        \n",
    "        self.attention_vector = nn.Parameter(\n",
    "            torch.empty(self.hidden_dim, dtype=torch.float32),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        nn.init.xavier_normal_(self.attention_vector.unsqueeze(-1))\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor\n",
    "    ):\n",
    "        logits = x.matmul(self.attention_vector)\n",
    "        ai = (logits - logits.max()).exp()\n",
    "\n",
    "        ai = ai * mask\n",
    "        att_weights = ai / (ai.sum(dim=1, keepdim=True) + 1e-12)\n",
    "        weighted_input = x * att_weights.unsqueeze(-1)\n",
    "        output = weighted_input.sum(dim=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return output, att_weights\n",
    "        else:\n",
    "            return output, None\n",
    "        \n",
    "\n",
    "CONV_FEAT_SIZE = 16\n",
    "CONV_DELS = [1,2,4,8,16,32]\n",
    "\n",
    "class EvrazModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embs0 = nn.Embedding(3, 3)\n",
    "        self.embs1 = nn.Embedding(2, 3)\n",
    "        self.embs2 = nn.Embedding(4, 3)\n",
    "        self.embs3 = nn.Embedding(29, 3)\n",
    "        self.embs4 = nn.Embedding(4, 3)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                torch.nn.Conv1d(self.get_conv_size(i), CONV_FEAT_SIZE, 2, stride=1, \n",
    "                                padding=0, dilation=d, bias=False),\n",
    "                nn.BatchNorm1d(CONV_FEAT_SIZE),\n",
    "                nn.SiLU()\n",
    "            )\n",
    "            for i,d in enumerate(CONV_DELS)\n",
    "        ])\n",
    "        self.conv_attn = AttentionWeightedAverage(CONV_FEAT_SIZE)\n",
    "                \n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(CONV_FEAT_SIZE+len(static_num_feats)+3*4),\n",
    "            nn.Linear(CONV_FEAT_SIZE+len(static_num_feats)+3*4, 8),\n",
    "            nn.LayerNorm(8),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.LayerNorm(8),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(8,2)\n",
    "        )\n",
    "        \n",
    "    def get_conv_size(self, i):\n",
    "        if i == 0:\n",
    "            return len(ts_num_feats)+3\n",
    "        return CONV_FEAT_SIZE\n",
    "    \n",
    "    def forward(self, static_cat, static_numeric, ts_cat, ts_numeric):\n",
    "        x0 = self.embs0(static_cat[:,0])\n",
    "        x1 = self.embs1(static_cat[:,1])\n",
    "        x2 = self.embs2(static_cat[:,2])\n",
    "        x3 = self.embs3(static_cat[:,3])\n",
    "        x4 = static_numeric\n",
    "        \n",
    "        y0 = self.embs4(ts_cat[:,:,0])\n",
    "        y = torch.cat([y0,ts_numeric], dim=-1).transpose(1,2)\n",
    "        \n",
    "        for m in self.convs:\n",
    "            y = m(y)\n",
    "        \n",
    "        y = y.transpose(1,2)\n",
    "        y = self.conv_attn(y, torch.ones((y.shape[0], y.shape[1])).to(y.device))[0]\n",
    "        \n",
    "        x = torch.cat([y,x0,x1,x2,x3,x4], dim=-1)\n",
    "        \n",
    "        x[torch.isnan(x)] = 0\n",
    "        x[torch.isinf(x)] = 0\n",
    "        \n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d363ded1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EvrazModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "96ec3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = EvrazDatasetV(df_train, df_train_static)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9a1d9034",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dl:\n",
    "    out = model(x[\"static_cat\"].long().to(device),\n",
    "          x['static_numeric'].float().to(device),\n",
    "          x['ts_cat'].long().to(device),\n",
    "          x['ts_numeric'].float().to(device)\n",
    "         )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a1d310dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2516,  0.0375]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41ecf429",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(answers, user_csv):\n",
    "    delta_c = np.abs(np.array(answers[:,1]) - np.array(user_csv[:,1]))\n",
    "    hit_rate_c = np.int64(delta_c < 0.02)\n",
    "\n",
    "    delta_t = np.abs(np.array(answers[:,0]) - np.array(user_csv[:,0]))\n",
    "    hit_rate_t = np.int64(delta_t < 20)\n",
    "\n",
    "    N = np.size(answers[:,0])\n",
    "\n",
    "    return np.sum(hit_rate_c + hit_rate_t) / 2 / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f5d043b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=239)\n",
    "\n",
    "nplvs = df_train_static.NPLV.unique()\n",
    "\n",
    "for vipl_train, vipl_valid in kf.split(nplvs):\n",
    "    break\n",
    "nplv_train = [nplvs[x] for x in vipl_train if nplvs[x] not in [511156,512299]]\n",
    "nplv_valid = [nplvs[x] for x in vipl_valid if nplvs[x] not in [511156,512299]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c340d590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5422705314009661\n",
      "3 0.46980676328502413\n",
      "4 0.48792270531400966\n",
      "5 0.501207729468599\n",
      "6 0.48792270531400966\n",
      "7 0.45410628019323673\n",
      "8 0.4758454106280193\n",
      "9 0.48429951690821255\n",
      "10 0.4504830917874396\n",
      "11 0.4963768115942029\n",
      "12 0.48188405797101447\n",
      "13 0.4758454106280193\n",
      "14 0.4830917874396135\n",
      "15 0.4855072463768116\n",
      "16 0.46859903381642515\n",
      "17 0.4504830917874396\n",
      "18 0.3864734299516908\n",
      "19 0.46980676328502413\n",
      "20 0.45410628019323673\n",
      "21 0.46980676328502413\n",
      "22 0.45169082125603865\n",
      "23 0.4444444444444444\n",
      "24 0.4601449275362319\n",
      "25 0.4227053140096618\n",
      "26 0.4528985507246377\n",
      "27 0.4384057971014493\n",
      "28 0.46497584541062803\n",
      "29 0.41183574879227053\n",
      "30 0.42632850241545894\n",
      "31 0.42995169082125606\n",
      "32 0.4251207729468599\n",
      "33 0.40458937198067635\n",
      "34 0.43719806763285024\n",
      "35 0.4601449275362319\n",
      "36 0.4033816425120773\n",
      "37 0.3973429951690821\n",
      "38 0.40700483091787437\n",
      "39 0.4166666666666667\n",
      "40 0.4504830917874396\n",
      "41 0.4214975845410628\n",
      "42 0.4227053140096618\n",
      "43 0.3864734299516908\n",
      "44 0.29589371980676327\n",
      "45 0.42028985507246375\n",
      "46 0.3695652173913043\n",
      "47 0.36835748792270534\n",
      "48 0.39855072463768115\n",
      "49 0.36231884057971014\n",
      "50 0.40458937198067635\n",
      "51 0.3719806763285024\n",
      "52 0.3961352657004831\n",
      "53 0.3828502415458937\n",
      "54 0.37077294685990336\n",
      "55 0.38405797101449274\n",
      "56 0.38164251207729466\n",
      "57 0.44806763285024154\n",
      "58 0.44806763285024154\n",
      "59 0.3647342995169082\n",
      "60 0.3466183574879227\n",
      "61 0.40942028985507245\n",
      "62 0.38768115942028986\n",
      "63 0.35990338164251207\n",
      "64 0.428743961352657\n",
      "65 0.3743961352657005\n",
      "66 0.42391304347826086\n",
      "67 0.43478260869565216\n",
      "68 0.4214975845410628\n",
      "69 0.3756038647342995\n",
      "70 0.4166666666666667\n",
      "71 0.39371980676328505\n",
      "72 0.4142512077294686\n",
      "73 0.39371980676328505\n",
      "74 0.42995169082125606\n",
      "75 0.3852657004830918\n",
      "76 0.40217391304347827\n",
      "77 0.3611111111111111\n",
      "78 0.3780193236714976\n",
      "79 0.4082125603864734\n",
      "80 0.3804347826086957\n",
      "81 0.42632850241545894\n",
      "82 0.4033816425120773\n",
      "83 0.39009661835748793\n",
      "84 0.3756038647342995\n",
      "85 0.4166666666666667\n",
      "86 0.40217391304347827\n",
      "87 0.3756038647342995\n",
      "88 0.4166666666666667\n",
      "89 0.43719806763285024\n",
      "90 0.4106280193236715\n",
      "91 0.4396135265700483\n",
      "92 0.392512077294686\n",
      "93 0.427536231884058\n",
      "94 0.39492753623188404\n",
      "95 0.38768115942028986\n",
      "96 0.3997584541062802\n",
      "97 0.42632850241545894\n",
      "98 0.4033816425120773\n",
      "99 0.4190821256038647\n",
      "100 0.4142512077294686\n",
      "101 0.40458937198067635\n",
      "102 0.45169082125603865\n",
      "103 0.4468599033816425\n",
      "104 0.40700483091787437\n",
      "105 0.40458937198067635\n",
      "106 0.46618357487922707\n",
      "107 0.4057971014492754\n",
      "108 0.427536231884058\n",
      "109 0.39855072463768115\n",
      "110 0.4190821256038647\n",
      "111 0.43719806763285024\n",
      "112 0.427536231884058\n",
      "113 0.41183574879227053\n",
      "114 0.3961352657004831\n",
      "115 0.44082125603864736\n",
      "116 0.3864734299516908\n",
      "117 0.322463768115942\n",
      "118 0.44082125603864736\n",
      "119 0.42028985507246375\n",
      "120 0.427536231884058\n",
      "121 0.4396135265700483\n",
      "122 0.4214975845410628\n",
      "123 0.42632850241545894\n",
      "124 0.4251207729468599\n",
      "125 0.38405797101449274\n",
      "126 0.4251207729468599\n",
      "127 0.3635265700483092\n",
      "128 0.4504830917874396\n",
      "129 0.40700483091787437\n",
      "130 0.4758454106280193\n",
      "131 0.3333333333333333\n",
      "132 0.4492753623188406\n",
      "133 0.42632850241545894\n",
      "134 0.37318840579710144\n",
      "135 0.45652173913043476\n",
      "136 0.391304347826087\n",
      "137 0.4577294685990338\n",
      "138 0.428743961352657\n",
      "139 0.3973429951690821\n",
      "140 0.4384057971014493\n",
      "141 0.42995169082125606\n",
      "142 0.3997584541062802\n",
      "143 0.3997584541062802\n",
      "144 0.44323671497584544\n",
      "145 0.3393719806763285\n",
      "146 0.45893719806763283\n",
      "147 0.13647342995169082\n",
      "148 0.40096618357487923\n",
      "149 0.4227053140096618\n",
      "150 0.41183574879227053\n",
      "151 0.4468599033816425\n",
      "152 0.3635265700483092\n",
      "153 0.40942028985507245\n",
      "154 0.39371980676328505\n",
      "155 0.4190821256038647\n",
      "156 0.4335748792270531\n",
      "157 0.40700483091787437\n",
      "158 0.4178743961352657\n",
      "159 0.40942028985507245\n",
      "160 0.42995169082125606\n",
      "161 0.46618357487922707\n",
      "162 0.42995169082125606\n",
      "163 0.4335748792270531\n",
      "164 0.4311594202898551\n",
      "165 0.44323671497584544\n",
      "166 0.4251207729468599\n",
      "167 0.34903381642512077\n",
      "168 0.4251207729468599\n",
      "169 0.428743961352657\n",
      "170 0.46980676328502413\n",
      "171 0.49033816425120774\n",
      "172 0.3502415458937198\n",
      "173 0.40942028985507245\n",
      "174 0.3804347826086957\n",
      "175 0.4528985507246377\n",
      "176 0.4420289855072464\n",
      "177 0.48429951690821255\n",
      "178 0.4214975845410628\n",
      "179 0.3671497584541063\n",
      "180 0.4867149758454106\n",
      "181 0.48792270531400966\n",
      "182 0.42028985507246375\n",
      "183 0.4178743961352657\n",
      "184 0.37681159420289856\n",
      "185 0.44565217391304346\n",
      "186 0.4190821256038647\n",
      "187 0.3647342995169082\n",
      "188 0.44082125603864736\n",
      "189 0.42391304347826086\n",
      "190 0.49033816425120774\n",
      "191 0.4057971014492754\n",
      "192 0.4782608695652174\n",
      "193 0.41304347826086957\n",
      "194 0.4577294685990338\n",
      "195 0.46256038647342995\n",
      "196 0.46256038647342995\n",
      "197 0.3973429951690821\n",
      "198 0.45410628019323673\n",
      "199 0.40458937198067635\n",
      "200 0.4673913043478261\n",
      "201 0.3973429951690821\n",
      "202 0.47101449275362317\n",
      "203 0.40700483091787437\n",
      "204 0.4420289855072464\n",
      "205 0.4082125603864734\n",
      "206 0.4553140096618358\n",
      "207 0.48792270531400966\n",
      "208 0.4444444444444444\n",
      "209 0.4722222222222222\n",
      "210 0.463768115942029\n",
      "211 0.4166666666666667\n",
      "212 0.4468599033816425\n",
      "213 0.4396135265700483\n",
      "214 0.3852657004830918\n",
      "215 0.4492753623188406\n",
      "216 0.45169082125603865\n",
      "217 0.4190821256038647\n",
      "218 0.4613526570048309\n",
      "219 0.42028985507246375\n",
      "220 0.46618357487922707\n",
      "221 0.40217391304347827\n",
      "222 0.4227053140096618\n",
      "223 0.46256038647342995\n",
      "224 0.4033816425120773\n",
      "225 0.44565217391304346\n",
      "226 0.44806763285024154\n",
      "227 0.46497584541062803\n",
      "228 0.44082125603864736\n",
      "229 0.4492753623188406\n",
      "230 0.4384057971014493\n",
      "231 0.42632850241545894\n",
      "232 0.46980676328502413\n",
      "233 0.45169082125603865\n",
      "234 0.4190821256038647\n",
      "235 0.42995169082125606\n",
      "236 0.4782608695652174\n",
      "237 0.4359903381642512\n",
      "238 0.45410628019323673\n",
      "239 0.4227053140096618\n",
      "240 0.44082125603864736\n",
      "241 0.4553140096618358\n",
      "242 0.4311594202898551\n",
      "243 0.4420289855072464\n",
      "244 0.4420289855072464\n",
      "245 0.43478260869565216\n",
      "246 0.4528985507246377\n",
      "247 0.4311594202898551\n",
      "248 0.40942028985507245\n",
      "249 0.45169082125603865\n",
      "250 0.4359903381642512\n",
      "251 0.4251207729468599\n",
      "252 0.4384057971014493\n",
      "253 0.45652173913043476\n",
      "254 0.43719806763285024\n",
      "255 0.40700483091787437\n",
      "256 0.4746376811594203\n",
      "257 0.4468599033816425\n",
      "258 0.4251207729468599\n",
      "259 0.35990338164251207\n",
      "260 0.3780193236714976\n",
      "261 0.4106280193236715\n",
      "262 0.45169082125603865\n",
      "263 0.4528985507246377\n",
      "264 0.4227053140096618\n",
      "265 0.4214975845410628\n",
      "266 0.392512077294686\n",
      "267 0.4082125603864734\n",
      "268 0.45410628019323673\n",
      "269 0.44806763285024154\n",
      "270 0.4190821256038647\n",
      "271 0.42995169082125606\n",
      "272 0.4468599033816425\n",
      "273 0.427536231884058\n",
      "274 0.40096618357487923\n",
      "275 0.45169082125603865\n",
      "276 0.4190821256038647\n",
      "277 0.4311594202898551\n",
      "278 0.4384057971014493\n",
      "279 0.4746376811594203\n",
      "280 0.43719806763285024\n",
      "281 0.4082125603864734\n",
      "282 0.46497584541062803\n",
      "283 0.4492753623188406\n",
      "284 0.44323671497584544\n",
      "285 0.4335748792270531\n",
      "286 0.13526570048309178\n",
      "287 0.4396135265700483\n",
      "288 0.4178743961352657\n",
      "289 0.4492753623188406\n",
      "290 0.4553140096618358\n",
      "291 0.4106280193236715\n",
      "292 0.39492753623188404\n",
      "293 0.41545893719806765\n",
      "294 0.43719806763285024\n",
      "295 0.44806763285024154\n",
      "296 0.45893719806763283\n",
      "297 0.428743961352657\n",
      "298 0.40942028985507245\n",
      "299 0.4033816425120773\n"
     ]
    }
   ],
   "source": [
    "train_ds = EvrazDatasetV(df_train.loc[df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=1, num_workers=0)\n",
    "valid_ds = EvrazDatasetV(df_train.loc[~df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[~df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "valid_dl = DataLoader(valid_ds, shuffle=False, batch_size=1, num_workers=0)\n",
    "\n",
    "model = EvrazModel().to(device)\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.0)\n",
    "\n",
    "\n",
    "model.train();\n",
    "optimizer.zero_grad()\n",
    "\n",
    "best_metric = 0\n",
    "for epoch in range(300):\n",
    "    losses = []\n",
    "    for x in train_dl:\n",
    "        out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "             )\n",
    "        loss1 = criterion1(out[:,0], x['y'].float().to(device)[:,0])\n",
    "        loss2 = criterion2(out[:,1], x['y'].float().to(device)[:,1])\n",
    "        loss = loss1*0.5+loss2\n",
    "        #losses.append(loss.item())\n",
    "        #if len(losses) == 100:\n",
    "        #    print(torch.mean(torch.tensor(losses)))\n",
    "        #    losses = []\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    outs, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x in valid_dl:\n",
    "            out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "            )\n",
    "            ys.append(x['y'].numpy())\n",
    "            outs.append(out.detach().cpu().numpy())\n",
    "    \n",
    "    outs = np.concatenate(outs, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    \n",
    "    outs = target_scaler.inverse_transform(outs)\n",
    "    ys = target_scaler.inverse_transform(ys)\n",
    "    \n",
    "    m = metric(outs, ys)\n",
    "    print(epoch, m)\n",
    "    if m > best_metric:\n",
    "        torch.save(model.state_dict(), 'model0.pth')\n",
    "        best_metric = m\n",
    "    model.train()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4f51e7e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5433734939759036\n",
      "1 0.5433734939759036\n",
      "2 0.5566265060240964\n",
      "3 0.5662650602409639\n",
      "4 0.5481927710843374\n",
      "5 0.5650602409638554\n",
      "6 0.5650602409638554\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_287456/977011107.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         out = model(x[\"static_cat\"].long().to(device),\n\u001b[1;32m     31\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'static_numeric'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_287456/1540403550.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     21\u001b[0m                 \u001b[0;34m\"static_numeric\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNPLV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_num_feats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;34m\"ts_cat\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNPLV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_cat_feats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m                 \u001b[0;34m\"ts_numeric\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNPLV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_num_feats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0;34m\"y\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNPLV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_fts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                }\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/ops/common.py\u001b[0m in \u001b[0;36mnew_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/arraylike.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__eq__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cmp_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"__ne__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_cmp_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   5500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5501\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"ignore\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5502\u001b[0;31m             \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomparison_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5503\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres_values\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mres_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36mcomparison_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m         \u001b[0mres_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    289\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/ops/array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    167\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mis_object_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_cmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_numexpr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;31m# error: \"None\" not callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/computation/expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_TEST_MODE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0m_store_test_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for vipl_train, vipl_valid in kf.split(nplvs):\n",
    "    if i == 1:\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "nplv_train = [nplvs[x] for x in vipl_train if nplvs[x] not in [511156,512299]]\n",
    "nplv_valid = [nplvs[x] for x in vipl_valid if nplvs[x] not in [511156,512299]]\n",
    "\n",
    "train_ds = EvrazDatasetV(df_train.loc[df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=1, num_workers=0)\n",
    "valid_ds = EvrazDatasetV(df_train.loc[~df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[~df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "valid_dl = DataLoader(valid_ds, shuffle=False, batch_size=1, num_workers=0)\n",
    "\n",
    "model = EvrazModel().to(device)\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
    "\n",
    "model.train();\n",
    "optimizer.zero_grad()\n",
    "\n",
    "best_metric = 0\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    for x in train_dl:\n",
    "        out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "             )\n",
    "        loss1 = criterion1(out[:,0], x['y'].float().to(device)[:,0])\n",
    "        loss2 = criterion2(out[:,1], x['y'].float().to(device)[:,1])\n",
    "        loss = loss1*0.5+loss2\n",
    "        #losses.append(loss.item())\n",
    "        #if len(losses) == 100:\n",
    "        #    print(torch.mean(torch.tensor(losses)))\n",
    "        #    losses = []\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    outs, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x in valid_dl:\n",
    "            out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "            )\n",
    "            ys.append(x['y'].numpy())\n",
    "            outs.append(out.detach().cpu().numpy())\n",
    "    \n",
    "    outs = np.concatenate(outs, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    \n",
    "    outs = target_scaler.inverse_transform(outs)\n",
    "    ys = target_scaler.inverse_transform(ys)\n",
    "    \n",
    "    m = metric(outs, ys)\n",
    "    print(epoch, m)\n",
    "    if m > best_metric:\n",
    "        torch.save(model.state_dict(), 'model1.pth')\n",
    "        best_metric = m\n",
    "    model.train()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c0df8384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5205314009661836\n",
      "1 0.5289855072463768\n",
      "2 0.5289855072463768\n",
      "3 0.532608695652174\n",
      "4 0.533816425120773\n",
      "5 0.532608695652174\n",
      "6 0.5072463768115942\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_287456/3212136937.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         out = model(x[\"static_cat\"].long().to(device),\n\u001b[1;32m     30\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'static_numeric'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_287456/1540403550.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnlmv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlmv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         return {\"static_cat\": self.static_train_df.loc[self.static_train_df.NPLV == nlmv, static_cat_feats].fillna(0).values[0],\n\u001b[0;32m---> 21\u001b[0;31m                 \u001b[0;34m\"static_numeric\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNPLV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_num_feats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m                 \u001b[0;34m\"ts_cat\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNPLV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_cat_feats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0;34m\"ts_numeric\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNPLV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_num_feats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    804\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m             \u001b[0;31m# We should never have retval.ndim < self.ndim, as that should\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0;31m#  be handled by the _getitem_lowerdim call above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1151\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cannot index with multidimensional key\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1153\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m             \u001b[0;31m# nested tuple slicing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1092\u001b[0m         \u001b[0;31m# A collection of keys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1093\u001b[0m         \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1094\u001b[0;31m         return self.obj._reindex_with_indexers(\n\u001b[0m\u001b[1;32m   1095\u001b[0m             \u001b[0;34m{\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1096\u001b[0m         )\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[0;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[1;32m   4881\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4882\u001b[0m             \u001b[0;31m# TODO: speed up on homogeneous DataFrame objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4883\u001b[0;31m             new_data = new_data.reindex_indexer(\n\u001b[0m\u001b[1;32m   4884\u001b[0m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4885\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy, consolidate, only_slice)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m             new_blocks = self._slice_take_blocks_ax0(\n\u001b[0m\u001b[1;32m    677\u001b[0m                 \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monly_slice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0monly_slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m             )\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m_slice_take_blocks_ax0\u001b[0;34m(self, slice_or_indexer, fill_value, only_slice)\u001b[0m\n\u001b[1;32m    799\u001b[0m                     \u001b[0;31m# GH#32779 to avoid the performance penalty of copying,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    800\u001b[0m                     \u001b[0;31m#  we may try to only slice\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 801\u001b[0;31m                     \u001b[0mtaker\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mblklocs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    802\u001b[0m                     \u001b[0mmax_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr_locs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    803\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0monly_slice\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for vipl_train, vipl_valid in kf.split(nplvs):\n",
    "    if i == 2:\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "nplv_train = [nplvs[x] for x in vipl_train if nplvs[x] not in [511156,512299]]\n",
    "nplv_valid = [nplvs[x] for x in vipl_valid if nplvs[x] not in [511156,512299]]\n",
    "\n",
    "train_ds = EvrazDatasetV(df_train.loc[df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=1, num_workers=0)\n",
    "valid_ds = EvrazDatasetV(df_train.loc[~df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[~df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "valid_dl = DataLoader(valid_ds, shuffle=False, batch_size=1, num_workers=0)\n",
    "\n",
    "model = EvrazModel().to(device)\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
    "\n",
    "model.train();\n",
    "optimizer.zero_grad()\n",
    "\n",
    "best_metric = 0\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    for x in train_dl:\n",
    "        out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "             )\n",
    "        loss1 = criterion1(out[:,0], x['y'].float().to(device)[:,0])\n",
    "        loss2 = criterion2(out[:,1], x['y'].float().to(device)[:,1])\n",
    "        loss = loss1*0.5+loss2\n",
    "        #losses.append(loss.item())\n",
    "        #if len(losses) == 100:\n",
    "        #    print(torch.mean(torch.tensor(losses)))\n",
    "        #    losses = []\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    outs, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x in valid_dl:\n",
    "            out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "            )\n",
    "            ys.append(x['y'].numpy())\n",
    "            outs.append(out.detach().cpu().numpy())\n",
    "    \n",
    "    outs = np.concatenate(outs, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    \n",
    "    outs = target_scaler.inverse_transform(outs)\n",
    "    ys = target_scaler.inverse_transform(ys)\n",
    "    \n",
    "    m = metric(outs, ys)\n",
    "    print(epoch, m)\n",
    "    if m > best_metric:\n",
    "        torch.save(model.state_dict(), 'model2.pth')\n",
    "        best_metric = m\n",
    "    model.train()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "56e34bf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5531400966183575\n",
      "1 0.5591787439613527\n",
      "2 0.5881642512077294\n",
      "3 0.5664251207729468\n",
      "4 0.5797101449275363\n",
      "5 0.5555555555555556\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_287456/4172528715.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         out = model(x[\"static_cat\"].long().to(device),\n\u001b[0m\u001b[1;32m     30\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'static_numeric'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ts_cat'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_287456/479688857.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, static_cat, static_numeric, ts_cat, ts_numeric)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mused\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mnormalization\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32min\u001b[0m \u001b[0meval\u001b[0m \u001b[0mmode\u001b[0m \u001b[0mwhen\u001b[0m \u001b[0mbuffers\u001b[0m \u001b[0mare\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \"\"\"\n\u001b[0;32m--> 168\u001b[0;31m         return F.batch_norm(\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# If buffers are not to be tracked, ensure that they won't be updated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2280\u001b[0m         \u001b[0m_verify_batch_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2282\u001b[0;31m     return torch.batch_norm(\n\u001b[0m\u001b[1;32m   2283\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2284\u001b[0m     )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for vipl_train, vipl_valid in kf.split(nplvs):\n",
    "    if i == 3:\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "nplv_train = [nplvs[x] for x in vipl_train if nplvs[x] not in [511156,512299]]\n",
    "nplv_valid = [nplvs[x] for x in vipl_valid if nplvs[x] not in [511156,512299]]\n",
    "\n",
    "train_ds = EvrazDatasetV(df_train.loc[df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=1, num_workers=0)\n",
    "valid_ds = EvrazDatasetV(df_train.loc[~df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[~df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "valid_dl = DataLoader(valid_ds, shuffle=False, batch_size=1, num_workers=0)\n",
    "\n",
    "model = EvrazModel().to(device)\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
    "\n",
    "model.train();\n",
    "optimizer.zero_grad()\n",
    "\n",
    "best_metric = 0\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    for x in train_dl:\n",
    "        out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "             )\n",
    "        loss1 = criterion1(out[:,0], x['y'].float().to(device)[:,0])\n",
    "        loss2 = criterion2(out[:,1], x['y'].float().to(device)[:,1])\n",
    "        loss = loss1*0.5+loss2\n",
    "        #losses.append(loss.item())\n",
    "        #if len(losses) == 100:\n",
    "        #    print(torch.mean(torch.tensor(losses)))\n",
    "        #    losses = []\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    outs, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x in valid_dl:\n",
    "            out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "            )\n",
    "            ys.append(x['y'].numpy())\n",
    "            outs.append(out.detach().cpu().numpy())\n",
    "    \n",
    "    outs = np.concatenate(outs, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    \n",
    "    outs = target_scaler.inverse_transform(outs)\n",
    "    ys = target_scaler.inverse_transform(ys)\n",
    "    \n",
    "    m = metric(outs, ys)\n",
    "    print(epoch, m)\n",
    "    if m > best_metric:\n",
    "        torch.save(model.state_dict(), 'model3.pth')\n",
    "        best_metric = m\n",
    "    model.train()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "af7eb18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5205314009661836\n",
      "1 0.535024154589372\n",
      "2 0.44323671497584544\n",
      "3 0.5193236714975845\n",
      "4 0.5398550724637681\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_287456/1483046904.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_dl\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         out = model(x[\"static_cat\"].long().to(device),\n\u001b[1;32m     30\u001b[0m               \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'static_numeric'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_287456/1540403550.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mnlmv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlmv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         return {\"static_cat\": self.static_train_df.loc[self.static_train_df.NPLV == nlmv, static_cat_feats].fillna(0).values[0],\n\u001b[0m\u001b[1;32m     21\u001b[0m                 \u001b[0;34m\"static_numeric\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatic_train_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNPLV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatic_num_feats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;34m\"ts_cat\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeat_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNPLV\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnlmv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mts_cat_feats\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    923\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mKeyError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtakeable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_takeable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m             \u001b[0;31m# we by definition only have the 0th axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m   1107\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_multi_take\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1109\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_tuple_same_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtup\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_tuple_same_dim\u001b[0;34m(self, tup)\u001b[0m\n\u001b[1;32m    804\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 806\u001b[0;31m             \u001b[0mretval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    807\u001b[0m             \u001b[0;31m# We should never have retval.ndim < self.ndim, as that should\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    808\u001b[0m             \u001b[0;31m#  be handled by the _getitem_lowerdim call above.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1142\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_slice_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1144\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getbool_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1145\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mis_list_like_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_getbool_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0;31m# caller is responsible for ensuring non-None axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    949\u001b[0m         \u001b[0minds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_take_with_is_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36mcheck_bool_indexer\u001b[0;34m(index, key)\u001b[0m\n\u001b[1;32m   2399\u001b[0m         \u001b[0;31m# key may contain nan elements, check_array_indexer needs bool array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2400\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2401\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcheck_array_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/indexers.py\u001b[0m in \u001b[0;36mcheck_array_indexer\u001b[0;34m(array, indexer)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m         \u001b[0;31m# GH26658\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order, like)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_asarray_with_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlike\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlike\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.cache/pypoetry/virtualenvs/evrazai-ixJ54l6x-py3.8/lib/python3.8/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5471\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5472\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5473\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5474\u001b[0m         \"\"\"\n\u001b[1;32m   5475\u001b[0m         \u001b[0mAfter\u001b[0m \u001b[0mregular\u001b[0m \u001b[0mattribute\u001b[0m \u001b[0maccess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mtry\u001b[0m \u001b[0mlooking\u001b[0m \u001b[0mup\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for vipl_train, vipl_valid in kf.split(nplvs):\n",
    "    if i == 4:\n",
    "        break\n",
    "    i += 1\n",
    "    \n",
    "nplv_train = [nplvs[x] for x in vipl_train if nplvs[x] not in [511156,512299]]\n",
    "nplv_valid = [nplvs[x] for x in vipl_valid if nplvs[x] not in [511156,512299]]\n",
    "\n",
    "train_ds = EvrazDatasetV(df_train.loc[df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=1, num_workers=0)\n",
    "valid_ds = EvrazDatasetV(df_train.loc[~df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[~df_train_static.NPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "valid_dl = DataLoader(valid_ds, shuffle=False, batch_size=1, num_workers=0)\n",
    "\n",
    "model = EvrazModel().to(device)\n",
    "criterion1 = nn.L1Loss()\n",
    "criterion2 = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=0.001)\n",
    "\n",
    "model.train();\n",
    "optimizer.zero_grad()\n",
    "\n",
    "best_metric = 0\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    for x in train_dl:\n",
    "        out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "             )\n",
    "        loss1 = criterion1(out[:,0], x['y'].float().to(device)[:,0])\n",
    "        loss2 = criterion2(out[:,1], x['y'].float().to(device)[:,1])\n",
    "        loss = loss1*0.5+loss2\n",
    "        #losses.append(loss.item())\n",
    "        #if len(losses) == 100:\n",
    "        #    print(torch.mean(torch.tensor(losses)))\n",
    "        #    losses = []\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    outs, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x in valid_dl:\n",
    "            out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "            )\n",
    "            ys.append(x['y'].numpy())\n",
    "            outs.append(out.detach().cpu().numpy())\n",
    "    \n",
    "    outs = np.concatenate(outs, axis=0)\n",
    "    ys = np.concatenate(ys, axis=0)\n",
    "    \n",
    "    outs = target_scaler.inverse_transform(outs)\n",
    "    ys = target_scaler.inverse_transform(ys)\n",
    "    \n",
    "    m = metric(outs, ys)\n",
    "    print(epoch, m)\n",
    "    if m > best_metric:\n",
    "        torch.save(model.state_dict(), 'model4.pth')\n",
    "        best_metric = m\n",
    "    model.train()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ca05540d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test_static['TST'] = 0\n",
    "df_test_static['C'] = 0\n",
    "test_ds = EvrazDatasetV(df_test, df_test_static)\n",
    "test_dl = DataLoader(test_ds, shuffle=False, batch_size=1, num_workers=0)\n",
    "\n",
    "for i in range(5):\n",
    "    model.load_state_dict(torch.load('model'+str(i) + '.pth'))\n",
    "    model.eval()\n",
    "    outs = []\n",
    "    with torch.no_grad():\n",
    "        for x in test_dl:\n",
    "            out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "            )\n",
    "            outs.append(out.detach().cpu().numpy())\n",
    "    outs = np.concatenate(outs, axis=0)\n",
    "    outs = target_scaler.inverse_transform(outs)\n",
    "    df_test_static['TST'] += outs[:,0] / 5.0\n",
    "    df_test_static['C'] += outs[:,1] / 5.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa9fef3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "q = df_test_static[['NPLV','TST','C']].copy()\n",
    "q.to_csv('sub2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7c31661d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NPLV</th>\n",
       "      <th>TST</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512324</td>\n",
       "      <td>1651.794128</td>\n",
       "      <td>0.054295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>512327</td>\n",
       "      <td>1648.189636</td>\n",
       "      <td>0.058045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512328</td>\n",
       "      <td>1648.035522</td>\n",
       "      <td>0.058825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512331</td>\n",
       "      <td>1648.213440</td>\n",
       "      <td>0.058173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512333</td>\n",
       "      <td>1648.038147</td>\n",
       "      <td>0.058030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>513369</td>\n",
       "      <td>1649.587830</td>\n",
       "      <td>0.057576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>513370</td>\n",
       "      <td>1649.741425</td>\n",
       "      <td>0.057481</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>513371</td>\n",
       "      <td>1649.352600</td>\n",
       "      <td>0.057496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>513372</td>\n",
       "      <td>1649.949036</td>\n",
       "      <td>0.057389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>513374</td>\n",
       "      <td>1649.798187</td>\n",
       "      <td>0.057314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NPLV          TST         C\n",
       "0    512324  1651.794128  0.054295\n",
       "1    512327  1648.189636  0.058045\n",
       "2    512328  1648.035522  0.058825\n",
       "3    512331  1648.213440  0.058173\n",
       "4    512333  1648.038147  0.058030\n",
       "..      ...          ...       ...\n",
       "775  513369  1649.587830  0.057576\n",
       "776  513370  1649.741425  0.057481\n",
       "777  513371  1649.352600  0.057496\n",
       "778  513372  1649.949036  0.057389\n",
       "779  513374  1649.798187  0.057314\n",
       "\n",
       "[780 rows x 3 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "c0a350eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NPLV</th>\n",
       "      <th>TST</th>\n",
       "      <th>C</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>512324</td>\n",
       "      <td>1654.231567</td>\n",
       "      <td>0.047168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>512327</td>\n",
       "      <td>1652.980286</td>\n",
       "      <td>0.055645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>512328</td>\n",
       "      <td>1651.970215</td>\n",
       "      <td>0.058765</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>512331</td>\n",
       "      <td>1652.762573</td>\n",
       "      <td>0.056714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>512333</td>\n",
       "      <td>1652.773438</td>\n",
       "      <td>0.056729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>775</th>\n",
       "      <td>513369</td>\n",
       "      <td>1654.987061</td>\n",
       "      <td>0.055939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>513370</td>\n",
       "      <td>1654.961182</td>\n",
       "      <td>0.055748</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>777</th>\n",
       "      <td>513371</td>\n",
       "      <td>1654.581360</td>\n",
       "      <td>0.055791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>778</th>\n",
       "      <td>513372</td>\n",
       "      <td>1655.233032</td>\n",
       "      <td>0.055838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>513374</td>\n",
       "      <td>1654.995972</td>\n",
       "      <td>0.055481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>780 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       NPLV          TST         C\n",
       "0    512324  1654.231567  0.047168\n",
       "1    512327  1652.980286  0.055645\n",
       "2    512328  1651.970215  0.058765\n",
       "3    512331  1652.762573  0.056714\n",
       "4    512333  1652.773438  0.056729\n",
       "..      ...          ...       ...\n",
       "775  513369  1654.987061  0.055939\n",
       "776  513370  1654.961182  0.055748\n",
       "777  513371  1654.581360  0.055791\n",
       "778  513372  1655.233032  0.055838\n",
       "779  513374  1654.995972  0.055481\n",
       "\n",
       "[780 rows x 3 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e413e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
