{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd5f4088",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Oct 29 18:36:57 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla V100-SXM2...  Off  | 00000000:8B:00.0 Off |                    0 |\n",
      "| N/A   34C    P0    36W / 300W |   1385MiB / 32510MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A       751      G   /usr/lib/xorg/Xorg                  4MiB |\n",
      "|    0   N/A  N/A    254912      C   ...nPOJSRNW-py3.8/bin/python     1377MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "38fbdf8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "baba3da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc5ad3d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chronom_test.csv   gas_test.csv     plavki_train.csv\t   sip_train.csv\r\n",
      "chronom_train.csv  gas_train.csv    produv_test.csv\t   target_train.csv\r\n",
      "chugun_test.csv    lom_test.csv     produv_train.csv\r\n",
      "chugun_train.csv   lom_train.csv    sample_submission.csv\r\n",
      "datka.zip\t   plavki_test.csv  sip_test.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99c3c8fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NPLV</th>\n",
       "      <th>plavka_VR_NACH</th>\n",
       "      <th>plavka_VR_KON</th>\n",
       "      <th>plavka_NMZ</th>\n",
       "      <th>plavka_NAPR_ZAD</th>\n",
       "      <th>plavka_STFUT</th>\n",
       "      <th>plavka_TIPE_FUR</th>\n",
       "      <th>plavka_ST_FURM</th>\n",
       "      <th>plavka_TIPE_GOL</th>\n",
       "      <th>plavka_ST_GOL</th>\n",
       "      <th>TST</th>\n",
       "      <th>C</th>\n",
       "      <th>plav_seconds</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:11</td>\n",
       "      <td>2021-01-01 03:51:10</td>\n",
       "      <td>С255</td>\n",
       "      <td>МНЛЗ</td>\n",
       "      <td>971</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>11</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>11</td>\n",
       "      <td>1690</td>\n",
       "      <td>0.060</td>\n",
       "      <td>2579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510009</td>\n",
       "      <td>2021-01-01 04:00:44</td>\n",
       "      <td>2021-01-01 05:07:28</td>\n",
       "      <td>С255</td>\n",
       "      <td>МНЛЗ</td>\n",
       "      <td>972</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>12</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>12</td>\n",
       "      <td>1683</td>\n",
       "      <td>0.097</td>\n",
       "      <td>4004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>510010</td>\n",
       "      <td>2021-01-01 05:12:29</td>\n",
       "      <td>2021-01-01 06:00:53</td>\n",
       "      <td>Ст3пс/Э</td>\n",
       "      <td>Изл</td>\n",
       "      <td>973</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>13</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>13</td>\n",
       "      <td>1662</td>\n",
       "      <td>0.091</td>\n",
       "      <td>2904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>510011</td>\n",
       "      <td>2021-01-01 06:13:48</td>\n",
       "      <td>2021-01-01 07:08:39</td>\n",
       "      <td>Св-08А.z02</td>\n",
       "      <td>Изл</td>\n",
       "      <td>974</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>14</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>14</td>\n",
       "      <td>1609</td>\n",
       "      <td>0.410</td>\n",
       "      <td>3291</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510012</td>\n",
       "      <td>2021-01-01 07:13:44</td>\n",
       "      <td>2021-01-01 08:01:59</td>\n",
       "      <td>SC2M/ЭТ</td>\n",
       "      <td>МНЛС</td>\n",
       "      <td>975</td>\n",
       "      <td>цилиндрическая</td>\n",
       "      <td>15</td>\n",
       "      <td>5 сопловая</td>\n",
       "      <td>15</td>\n",
       "      <td>1682</td>\n",
       "      <td>0.120</td>\n",
       "      <td>2895</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     NPLV      plavka_VR_NACH       plavka_VR_KON       plavka_NMZ  \\\n",
       "0  510008 2021-01-01 03:08:11 2021-01-01 03:51:10  С255              \n",
       "1  510009 2021-01-01 04:00:44 2021-01-01 05:07:28  С255              \n",
       "2  510010 2021-01-01 05:12:29 2021-01-01 06:00:53  Ст3пс/Э           \n",
       "3  510011 2021-01-01 06:13:48 2021-01-01 07:08:39  Св-08А.z02        \n",
       "4  510012 2021-01-01 07:13:44 2021-01-01 08:01:59  SC2M/ЭТ           \n",
       "\n",
       "  plavka_NAPR_ZAD  plavka_STFUT plavka_TIPE_FUR  plavka_ST_FURM  \\\n",
       "0            МНЛЗ           971  цилиндрическая              11   \n",
       "1            МНЛЗ           972  цилиндрическая              12   \n",
       "2             Изл           973  цилиндрическая              13   \n",
       "3             Изл           974  цилиндрическая              14   \n",
       "4            МНЛС           975  цилиндрическая              15   \n",
       "\n",
       "        plavka_TIPE_GOL  plavka_ST_GOL   TST      C  plav_seconds  \n",
       "0  5 сопловая                       11  1690  0.060          2579  \n",
       "1  5 сопловая                       12  1683  0.097          4004  \n",
       "2  5 сопловая                       13  1662  0.091          2904  \n",
       "3  5 сопловая                       14  1609  0.410          3291  \n",
       "4  5 сопловая                       15  1682  0.120          2895  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_static = pd.read_csv(DATA_DIR + 'plavki_train.csv').drop_duplicates('NPLV').reset_index(drop=True)\n",
    "df_test_static = pd.read_csv(DATA_DIR + 'plavki_test.csv')\n",
    "\n",
    "target = pd.read_csv(DATA_DIR + 'target_train.csv')\n",
    "df_train_static = df_train_static.merge(target, on='NPLV', how='left')\n",
    "\n",
    "for plavki in [df_train_static, df_test_static]:\n",
    "    plavki['plavka_VR_NACH'] = pd.to_datetime(plavki['plavka_VR_NACH'])\n",
    "    plavki['plavka_VR_KON'] = pd.to_datetime(plavki['plavka_VR_KON'])\n",
    "    plavki['plav_seconds'] = (((plavki.plavka_VR_KON.values - plavki.plavka_VR_NACH.values))*1e-9).astype(int)\n",
    "df_train_static.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43a947b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_cat_feats = ['plavka_NAPR_ZAD', 'plavka_TIPE_FUR', 'plavka_TIPE_GOL', 'plavka_NMZ']\n",
    "static_num_feats = ['plavka_STFUT','plavka_ST_FURM','plav_seconds','plavka_ST_GOL']\n",
    "target_fts = ['TST','C']\n",
    "ts_cat_feats = ['NMSYP']\n",
    "ts_num_feats = ['V', 'T', 'O2', 'N2', 'H2', 'CO2', 'CO', 'AR',\n",
    "       'T фурмы 1', 'T фурмы 2', 'O2_pressure', 'RAS', 'POL', 'VDSYP',\n",
    "       'VSSYP', 'VES_ch', 'T_ch', 'SI_ch', 'MN_ch', 'S_ch', 'P_ch', 'CR_ch',\n",
    "       'NI_ch', 'CU_ch', 'V_ch', 'TI_ch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22603249",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_287456/1568610564.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  p['plav_seconds'] = int(((p.plavka_VR_KON.values[0] - p.plavka_VR_NACH.values[0])).item()*1e-9)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "510009\n",
      "510010\n",
      "510011\n",
      "510012\n",
      "510013\n",
      "510014\n",
      "510015\n",
      "510016\n",
      "510017\n",
      "510018\n",
      "510019\n",
      "510020\n",
      "510021\n",
      "510022\n",
      "510023\n",
      "510024\n",
      "510026\n",
      "510027\n",
      "510028\n",
      "510030\n",
      "510031\n",
      "510032\n",
      "510033\n",
      "510037\n",
      "510040\n",
      "510043\n",
      "510044\n",
      "510045\n",
      "510047\n",
      "510048\n",
      "510049\n",
      "510050\n",
      "510051\n",
      "510052\n",
      "510053\n",
      "510055\n",
      "510057\n",
      "510058\n",
      "510059\n",
      "510060\n",
      "510061\n",
      "510062\n",
      "510063\n",
      "510064\n",
      "510066\n",
      "510067\n",
      "510068\n",
      "510069\n",
      "510070\n",
      "510071\n",
      "510072\n",
      "510073\n",
      "510074\n",
      "510075\n",
      "510076\n",
      "510077\n",
      "510078\n",
      "510079\n",
      "510080\n",
      "510081\n",
      "510082\n",
      "510083\n",
      "510084\n",
      "510087\n",
      "510088\n",
      "510089\n",
      "510090\n",
      "510091\n",
      "510092\n",
      "510093\n",
      "510094\n",
      "510095\n",
      "510096\n",
      "510097\n",
      "510098\n",
      "510099\n",
      "510100\n",
      "510101\n",
      "510102\n",
      "510103\n",
      "510104\n",
      "510105\n",
      "510106\n",
      "510107\n",
      "510108\n",
      "510109\n",
      "510110\n",
      "510111\n",
      "510112\n",
      "510114\n",
      "510115\n",
      "510116\n",
      "510117\n",
      "510118\n",
      "510119\n",
      "510120\n",
      "510121\n",
      "510122\n",
      "510124\n",
      "510125\n",
      "510126\n",
      "510128\n",
      "510130\n",
      "510131\n",
      "510132\n",
      "510133\n",
      "510134\n",
      "510135\n",
      "510136\n",
      "510137\n",
      "510138\n",
      "510139\n",
      "510140\n",
      "510141\n",
      "510142\n",
      "510143\n",
      "510144\n",
      "510145\n",
      "510146\n",
      "510147\n",
      "510148\n",
      "510149\n",
      "510151\n",
      "510152\n",
      "510153\n",
      "510156\n",
      "510157\n",
      "510158\n",
      "510159\n",
      "510160\n",
      "510161\n",
      "510163\n",
      "510164\n",
      "510165\n",
      "510166\n",
      "510167\n",
      "510168\n",
      "510169\n",
      "510170\n",
      "510171\n",
      "510172\n",
      "510174\n",
      "510175\n",
      "510176\n",
      "510177\n",
      "510178\n",
      "510179\n",
      "510180\n",
      "510181\n",
      "510182\n",
      "510183\n",
      "510184\n",
      "510185\n",
      "510186\n",
      "510187\n",
      "510188\n",
      "510189\n",
      "510190\n",
      "510191\n",
      "510192\n",
      "510193\n",
      "510195\n",
      "510197\n",
      "510198\n",
      "510199\n",
      "510200\n",
      "510201\n",
      "510202\n",
      "510203\n",
      "510205\n",
      "510206\n",
      "510207\n",
      "510208\n",
      "510209\n",
      "510210\n",
      "510211\n",
      "510212\n",
      "510213\n",
      "510214\n",
      "510215\n",
      "510216\n",
      "510217\n",
      "510218\n",
      "510219\n",
      "510220\n",
      "510221\n",
      "510222\n",
      "510223\n",
      "510224\n",
      "510225\n",
      "510226\n",
      "510227\n",
      "510228\n",
      "510229\n",
      "510230\n",
      "510231\n",
      "510232\n",
      "510233\n",
      "510234\n",
      "510235\n",
      "510236\n",
      "510237\n",
      "510238\n",
      "510239\n",
      "510240\n",
      "510241\n",
      "510242\n",
      "510243\n",
      "510244\n",
      "510245\n",
      "510246\n",
      "510248\n",
      "510249\n",
      "510250\n",
      "510251\n",
      "510252\n",
      "510253\n",
      "510254\n",
      "510255\n",
      "510258\n",
      "510259\n",
      "510260\n",
      "510261\n",
      "510263\n",
      "510264\n",
      "510265\n",
      "510266\n",
      "510267\n",
      "510268\n",
      "510269\n",
      "510270\n",
      "510271\n",
      "510272\n",
      "510273\n",
      "510274\n",
      "510275\n",
      "510276\n",
      "510278\n",
      "510279\n",
      "510280\n",
      "510281\n",
      "510282\n",
      "510283\n",
      "510284\n",
      "510285\n",
      "510286\n",
      "510287\n",
      "510288\n",
      "510289\n",
      "510290\n",
      "510291\n",
      "510292\n",
      "510293\n",
      "510294\n",
      "510295\n",
      "510296\n",
      "510297\n",
      "510299\n",
      "510300\n",
      "510301\n",
      "510302\n",
      "510303\n",
      "510304\n",
      "510305\n",
      "510306\n",
      "510307\n",
      "510308\n",
      "510310\n",
      "510311\n",
      "510312\n",
      "510313\n",
      "510314\n",
      "510317\n",
      "510318\n",
      "510319\n",
      "510320\n",
      "510321\n",
      "510322\n",
      "510323\n",
      "510324\n",
      "510325\n",
      "510326\n",
      "510327\n",
      "510328\n",
      "510329\n",
      "510330\n",
      "510331\n",
      "510332\n",
      "510333\n",
      "510334\n",
      "510335\n",
      "510336\n",
      "510337\n",
      "510338\n",
      "510339\n",
      "510341\n",
      "510342\n",
      "510343\n",
      "510344\n",
      "510345\n",
      "510347\n",
      "510348\n",
      "510349\n",
      "510350\n",
      "510351\n",
      "510353\n",
      "510354\n",
      "510355\n",
      "510356\n",
      "510358\n",
      "510359\n",
      "510360\n",
      "510361\n",
      "510363\n",
      "510364\n",
      "510365\n",
      "510366\n",
      "510367\n",
      "510368\n",
      "510369\n",
      "510370\n",
      "510371\n",
      "510372\n",
      "510373\n",
      "510374\n",
      "510375\n",
      "510376\n",
      "510377\n",
      "510378\n",
      "510379\n",
      "510381\n",
      "510384\n",
      "510385\n",
      "510386\n",
      "510387\n",
      "510388\n",
      "510389\n",
      "510390\n",
      "510391\n",
      "510392\n",
      "510393\n",
      "510394\n",
      "510395\n",
      "510397\n",
      "510398\n",
      "510399\n",
      "510400\n",
      "510401\n",
      "510403\n",
      "510404\n",
      "510405\n",
      "510406\n",
      "510407\n",
      "510408\n",
      "510409\n",
      "510410\n",
      "510411\n",
      "510412\n",
      "510413\n",
      "510414\n",
      "510415\n",
      "510416\n",
      "510417\n",
      "510418\n",
      "510419\n",
      "510420\n",
      "510421\n",
      "510422\n",
      "510423\n",
      "510425\n",
      "510426\n",
      "510427\n",
      "510428\n",
      "510429\n",
      "510430\n",
      "510431\n",
      "510432\n",
      "510433\n",
      "510435\n",
      "510436\n",
      "510437\n",
      "510438\n",
      "510439\n",
      "510440\n",
      "510441\n",
      "510442\n",
      "510443\n",
      "510444\n",
      "510445\n",
      "510446\n",
      "510448\n",
      "510449\n",
      "510450\n",
      "510451\n",
      "510452\n",
      "510453\n",
      "510454\n",
      "510455\n",
      "510456\n",
      "510457\n",
      "510458\n",
      "510459\n",
      "510460\n",
      "510461\n",
      "510462\n",
      "510463\n",
      "510464\n",
      "510466\n",
      "510467\n",
      "510468\n",
      "510469\n",
      "510470\n",
      "510471\n",
      "510472\n",
      "510473\n",
      "510474\n",
      "510475\n",
      "510476\n",
      "510477\n",
      "510478\n",
      "510479\n",
      "510480\n",
      "510482\n",
      "510483\n",
      "510484\n",
      "510485\n",
      "510486\n",
      "510487\n",
      "510488\n",
      "510489\n",
      "510490\n",
      "510491\n",
      "510492\n",
      "510493\n",
      "510494\n",
      "510495\n",
      "510496\n",
      "510497\n",
      "510498\n",
      "510499\n",
      "510500\n",
      "510501\n",
      "510503\n",
      "510504\n",
      "510505\n",
      "510506\n",
      "510507\n",
      "510509\n",
      "510510\n",
      "510511\n",
      "510512\n",
      "510513\n",
      "510514\n",
      "510515\n",
      "510516\n",
      "510517\n",
      "510518\n",
      "510519\n",
      "510520\n",
      "510521\n",
      "510523\n",
      "510524\n",
      "510525\n",
      "510526\n",
      "510527\n",
      "510528\n",
      "510529\n",
      "510530\n",
      "510531\n",
      "510532\n",
      "510533\n",
      "510535\n",
      "510536\n",
      "510537\n",
      "510538\n",
      "510539\n",
      "510540\n",
      "510541\n",
      "510542\n",
      "510543\n",
      "510544\n",
      "510546\n",
      "510547\n",
      "510548\n",
      "510549\n",
      "510550\n",
      "510551\n",
      "510552\n",
      "510553\n",
      "510554\n",
      "510555\n",
      "510558\n",
      "510560\n",
      "510561\n",
      "510563\n",
      "510564\n",
      "510565\n",
      "510566\n",
      "510567\n",
      "510568\n",
      "510569\n",
      "510570\n",
      "510571\n",
      "510572\n",
      "510573\n",
      "510574\n",
      "510575\n",
      "510576\n",
      "510577\n",
      "510578\n",
      "510579\n",
      "510580\n",
      "510582\n",
      "510583\n",
      "510584\n",
      "510585\n",
      "510586\n",
      "510587\n",
      "510589\n",
      "510590\n",
      "510592\n",
      "510593\n",
      "510594\n",
      "510595\n",
      "510596\n",
      "510597\n",
      "510598\n",
      "510599\n",
      "510600\n",
      "510601\n",
      "510602\n",
      "510603\n",
      "510604\n",
      "510605\n",
      "510608\n",
      "510609\n",
      "510614\n",
      "510616\n",
      "510622\n",
      "510623\n",
      "510624\n",
      "510626\n",
      "510627\n",
      "510628\n",
      "510629\n",
      "510630\n",
      "510631\n",
      "510632\n",
      "510633\n",
      "510634\n",
      "510635\n",
      "510636\n",
      "510637\n",
      "510638\n",
      "510639\n",
      "510640\n",
      "510641\n",
      "510642\n",
      "510643\n",
      "510644\n",
      "510645\n",
      "510646\n",
      "510647\n",
      "510648\n",
      "510649\n",
      "510650\n",
      "510652\n",
      "510653\n",
      "510654\n",
      "510655\n",
      "510656\n",
      "510657\n",
      "510658\n",
      "510659\n",
      "510660\n",
      "510662\n",
      "510664\n",
      "510665\n",
      "510666\n",
      "510667\n",
      "510668\n",
      "510669\n",
      "510670\n",
      "510671\n",
      "510672\n",
      "510673\n",
      "510674\n",
      "510675\n",
      "510676\n",
      "510677\n",
      "510678\n",
      "510679\n",
      "510680\n",
      "510681\n",
      "510682\n",
      "510683\n",
      "510684\n",
      "510685\n",
      "510686\n",
      "510687\n",
      "510688\n",
      "510689\n",
      "510690\n",
      "510691\n",
      "510692\n",
      "510693\n",
      "510694\n",
      "510695\n",
      "510696\n",
      "510697\n",
      "510698\n",
      "510699\n",
      "510700\n",
      "510701\n",
      "510702\n",
      "510703\n",
      "510704\n",
      "510705\n",
      "510706\n",
      "510708\n",
      "510710\n",
      "510712\n",
      "510713\n",
      "510714\n",
      "510715\n",
      "510716\n",
      "510717\n",
      "510718\n",
      "510719\n",
      "510720\n",
      "510721\n",
      "510722\n",
      "510723\n",
      "510724\n",
      "510725\n",
      "510726\n",
      "510727\n",
      "510728\n",
      "510729\n",
      "510730\n",
      "510731\n",
      "510732\n",
      "510733\n",
      "510734\n",
      "510735\n",
      "510736\n",
      "510737\n",
      "510738\n",
      "510739\n",
      "510740\n",
      "510741\n",
      "510742\n",
      "510743\n",
      "510744\n",
      "510745\n",
      "510746\n",
      "510747\n",
      "510748\n",
      "510749\n",
      "510750\n",
      "510751\n",
      "510752\n",
      "510753\n",
      "510754\n",
      "510755\n",
      "510756\n",
      "510757\n",
      "510758\n",
      "510759\n",
      "510760\n",
      "510761\n",
      "510762\n",
      "510763\n",
      "510764\n",
      "510765\n",
      "510766\n",
      "510767\n",
      "510768\n",
      "510770\n",
      "510771\n",
      "510772\n",
      "510773\n",
      "510774\n",
      "510775\n",
      "510776\n",
      "510777\n",
      "510778\n",
      "510779\n",
      "510780\n",
      "510781\n",
      "510782\n",
      "510783\n",
      "510784\n",
      "510787\n",
      "510788\n",
      "510789\n",
      "510790\n",
      "510791\n",
      "510792\n",
      "510793\n",
      "510794\n",
      "510795\n",
      "510797\n",
      "510798\n",
      "510799\n",
      "510800\n",
      "510801\n",
      "510802\n",
      "510803\n",
      "510804\n",
      "510805\n",
      "510806\n",
      "510807\n",
      "510808\n",
      "510809\n",
      "510810\n",
      "510812\n",
      "510813\n",
      "510814\n",
      "510815\n",
      "510817\n",
      "510818\n",
      "510819\n",
      "510820\n",
      "510821\n",
      "510822\n",
      "510823\n",
      "510824\n",
      "510825\n",
      "510826\n",
      "510827\n",
      "510828\n",
      "510829\n",
      "510831\n",
      "510833\n",
      "510834\n",
      "510836\n",
      "510837\n",
      "510838\n",
      "510839\n",
      "510840\n",
      "510841\n",
      "510842\n",
      "510843\n",
      "510844\n",
      "510845\n",
      "510846\n",
      "510847\n",
      "510848\n",
      "510849\n",
      "510850\n",
      "510851\n",
      "510852\n",
      "510853\n",
      "510854\n",
      "510855\n",
      "510856\n",
      "510857\n",
      "510858\n",
      "510859\n",
      "510860\n",
      "510861\n",
      "510862\n",
      "510863\n",
      "510864\n",
      "510865\n",
      "510866\n",
      "510867\n",
      "510868\n",
      "510869\n",
      "510870\n",
      "510871\n",
      "510872\n",
      "510873\n",
      "510874\n",
      "510875\n",
      "510876\n",
      "510877\n",
      "510878\n",
      "510879\n",
      "510880\n",
      "510881\n",
      "510882\n",
      "510883\n",
      "510884\n",
      "510885\n",
      "510886\n",
      "510887\n",
      "510888\n",
      "510889\n",
      "510891\n",
      "510892\n",
      "510893\n",
      "510894\n",
      "510895\n",
      "510896\n",
      "510898\n",
      "510899\n",
      "510900\n",
      "510901\n",
      "510903\n",
      "510904\n",
      "510905\n",
      "510906\n",
      "510907\n",
      "510908\n",
      "510909\n",
      "510910\n",
      "510911\n",
      "510912\n",
      "510914\n",
      "510915\n",
      "510916\n",
      "510917\n",
      "510918\n",
      "510919\n",
      "510920\n",
      "510921\n",
      "510922\n",
      "510923\n",
      "510924\n",
      "510925\n",
      "510926\n",
      "510927\n",
      "510928\n",
      "510929\n",
      "510931\n",
      "510932\n",
      "510933\n",
      "510934\n",
      "510936\n",
      "510937\n",
      "510939\n",
      "510941\n",
      "510942\n",
      "510943\n",
      "510944\n",
      "510945\n",
      "510946\n",
      "510947\n",
      "510948\n",
      "510949\n",
      "510950\n",
      "510951\n",
      "510952\n",
      "510953\n",
      "510954\n",
      "510955\n",
      "510956\n",
      "510957\n",
      "510958\n",
      "510959\n",
      "510960\n",
      "510961\n",
      "510962\n",
      "510963\n",
      "510965\n",
      "510966\n",
      "510968\n",
      "510969\n",
      "510970\n",
      "510972\n",
      "510975\n",
      "510976\n",
      "510977\n",
      "510978\n",
      "510979\n",
      "510980\n",
      "510981\n",
      "510982\n",
      "510983\n",
      "510984\n",
      "510985\n",
      "510986\n",
      "510987\n",
      "510988\n",
      "510989\n",
      "510990\n",
      "510991\n",
      "510992\n",
      "510993\n",
      "510994\n",
      "510995\n",
      "510996\n",
      "510997\n",
      "510998\n",
      "510999\n",
      "511000\n",
      "511001\n",
      "511002\n",
      "511003\n",
      "511004\n",
      "511005\n",
      "511006\n",
      "511007\n",
      "511008\n",
      "511009\n",
      "511011\n",
      "511012\n",
      "511013\n",
      "511014\n",
      "511015\n",
      "511016\n",
      "511017\n",
      "511018\n",
      "511019\n",
      "511020\n",
      "511021\n",
      "511022\n",
      "511023\n",
      "511024\n",
      "511025\n",
      "511027\n",
      "511028\n",
      "511029\n",
      "511030\n",
      "511031\n",
      "511032\n",
      "511033\n",
      "511034\n",
      "511035\n",
      "511036\n",
      "511037\n",
      "511038\n",
      "511039\n",
      "511040\n",
      "511041\n",
      "511042\n",
      "511044\n",
      "511045\n",
      "511046\n",
      "511047\n",
      "511048\n",
      "511049\n",
      "511050\n",
      "511051\n",
      "511052\n",
      "511053\n",
      "511054\n",
      "511055\n",
      "511056\n",
      "511057\n",
      "511058\n",
      "511059\n",
      "511060\n",
      "511061\n",
      "511062\n",
      "511063\n",
      "511064\n",
      "511065\n",
      "511066\n",
      "511067\n",
      "511068\n",
      "511069\n",
      "511070\n",
      "511071\n",
      "511072\n",
      "511073\n",
      "511074\n",
      "511075\n",
      "511076\n",
      "511077\n",
      "511078\n",
      "511079\n",
      "511080\n",
      "511081\n",
      "511084\n",
      "511085\n",
      "511086\n",
      "511087\n",
      "511089\n",
      "511090\n",
      "511091\n",
      "511092\n",
      "511093\n",
      "511094\n",
      "511096\n",
      "511097\n",
      "511099\n",
      "511100\n",
      "511101\n",
      "511103\n",
      "511104\n",
      "511105\n",
      "511106\n",
      "511107\n",
      "511108\n",
      "511109\n",
      "511110\n",
      "511112\n",
      "511113\n",
      "511114\n",
      "511115\n",
      "511116\n",
      "511118\n",
      "511119\n",
      "511120\n",
      "511121\n",
      "511122\n",
      "511123\n",
      "511124\n",
      "511126\n",
      "511127\n",
      "511128\n",
      "511130\n",
      "511131\n",
      "511135\n",
      "511137\n",
      "511139\n",
      "511140\n",
      "511141\n",
      "511142\n",
      "511143\n",
      "511144\n",
      "511145\n",
      "511146\n",
      "511147\n",
      "511148\n",
      "511149\n",
      "511150\n",
      "511151\n",
      "511152\n",
      "511155\n",
      "511156\n",
      "511157\n",
      "511158\n",
      "511159\n",
      "511160\n",
      "511161\n",
      "511162\n",
      "511163\n",
      "511164\n",
      "511165\n",
      "511166\n",
      "511167\n",
      "511169\n",
      "511171\n",
      "511172\n",
      "511173\n",
      "511175\n",
      "511176\n",
      "511177\n",
      "511178\n",
      "511179\n",
      "511180\n",
      "511181\n",
      "511182\n",
      "511183\n",
      "511184\n",
      "511186\n",
      "511187\n",
      "511189\n",
      "511191\n",
      "511192\n",
      "511193\n",
      "511194\n",
      "511195\n",
      "511196\n",
      "511197\n",
      "511198\n",
      "511199\n",
      "511200\n",
      "511201\n",
      "511202\n",
      "511203\n",
      "511204\n",
      "511205\n",
      "511206\n",
      "511207\n",
      "511208\n",
      "511209\n",
      "511210\n",
      "511211\n",
      "511213\n",
      "511214\n",
      "511215\n",
      "511216\n",
      "511217\n",
      "511218\n",
      "511221\n",
      "511222\n",
      "511223\n",
      "511225\n",
      "511226\n",
      "511229\n",
      "511230\n",
      "511232\n",
      "511233\n",
      "511234\n",
      "511235\n",
      "511236\n",
      "511237\n",
      "511238\n",
      "511239\n",
      "511240\n",
      "511241\n",
      "511242\n",
      "511243\n",
      "511245\n",
      "511246\n",
      "511247\n",
      "511248\n",
      "511249\n",
      "511250\n",
      "511251\n",
      "511252\n",
      "511253\n",
      "511254\n",
      "511255\n",
      "511256\n",
      "511257\n",
      "511258\n",
      "511259\n",
      "511260\n",
      "511261\n",
      "511262\n",
      "511263\n",
      "511264\n",
      "511265\n",
      "511266\n",
      "511267\n",
      "511268\n",
      "511270\n",
      "511271\n",
      "511272\n",
      "511273\n",
      "511274\n",
      "511275\n",
      "511276\n",
      "511277\n",
      "511278\n",
      "511279\n",
      "511280\n",
      "511281\n",
      "511282\n",
      "511283\n",
      "511284\n",
      "511285\n",
      "511286\n",
      "511287\n",
      "511288\n",
      "511289\n",
      "511290\n",
      "511291\n",
      "511292\n",
      "511293\n",
      "511295\n",
      "511296\n",
      "511297\n",
      "511298\n",
      "511299\n",
      "511300\n",
      "511301\n",
      "511302\n",
      "511303\n",
      "511305\n",
      "511306\n",
      "511307\n",
      "511308\n",
      "511309\n",
      "511310\n",
      "511311\n",
      "511312\n",
      "511313\n",
      "511314\n",
      "511315\n",
      "511316\n",
      "511317\n",
      "511318\n",
      "511319\n",
      "511320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "511321\n",
      "511322\n",
      "511323\n",
      "511324\n",
      "511325\n",
      "511326\n",
      "511327\n",
      "511328\n",
      "511329\n",
      "511330\n",
      "511331\n",
      "511332\n",
      "511333\n",
      "511334\n",
      "511335\n",
      "511336\n"
     ]
    }
   ],
   "source": [
    "def make_dataset(data_dir, postfix = 'train'):\n",
    "    plavki = pd.read_csv(data_dir + 'plavki_'+postfix + '.csv')\n",
    "    plavki['plavka_VR_NACH'] = pd.to_datetime(plavki['plavka_VR_NACH'])\n",
    "    plavki['plavka_VR_KON'] = pd.to_datetime(plavki['plavka_VR_KON'])\n",
    "    gas = pd.read_csv(data_dir + 'gas_'+postfix + '.csv')\n",
    "    gas['Time'] = pd.to_datetime(gas['Time'])\n",
    "    produv = pd.read_csv(data_dir + 'produv_'+postfix + '.csv')\n",
    "    produv['SEC'] = pd.to_datetime(produv['SEC'])\n",
    "    sip = pd.read_csv(data_dir + 'sip_'+postfix + '.csv')\n",
    "    sip['DAT_OTD'] = pd.to_datetime(sip['DAT_OTD'])\n",
    "    chugun = pd.read_csv(data_dir + 'chugun_'+postfix + '.csv')\n",
    "    chugun.columns = ['NPLV'] + [x+'_ch' for x in chugun.columns.tolist()[1:-1]] + ['DATA_ZAMERA']\n",
    "    \n",
    "    dfs = []\n",
    "    for g,df in gas.groupby('NPLV'):\n",
    "        print(g)\n",
    "        p = plavki.loc[plavki.NPLV == g]\n",
    "        p['plav_seconds'] = int(((p.plavka_VR_KON.values[0] - p.plavka_VR_NACH.values[0])).item()*1e-9)\n",
    "        df1 = df.loc[df.Time <= p.plavka_VR_KON.values[0]]\n",
    "        ppdf = produv.loc[(produv.NPLV == g) & (produv.SEC <= p.plavka_VR_KON.values[0])].drop(columns=['NPLV'])\n",
    "        df1 = pd.merge_asof(df1, ppdf, left_on='Time', right_on='SEC').drop(columns=['SEC'])\n",
    "        psip = sip.loc[(produv.NPLV == g) & (sip.DAT_OTD <= p.plavka_VR_KON.values[0])].drop(columns=['NPLV'])\n",
    "        df1 = pd.merge_asof(df1, psip, left_on='Time', right_on='DAT_OTD').drop(columns=['DAT_OTD'])\n",
    "        df1 = df1.merge(chugun, on='NPLV', how='left')\n",
    "        df1 = df1.drop(columns = ['DATA_ZAMERA'])\n",
    "        dfs.append(df1)\n",
    "        #if len(dfs) > 100:\n",
    "        #    break\n",
    "        \n",
    "    df = pd.concat(dfs, ignore_index = True).reset_index(drop=True)\n",
    "    df[ts_num_feats] = df[ts_num_feats].interpolate()\n",
    "    return df\n",
    "\n",
    "df_train = make_dataset(DATA_DIR, 'train')\n",
    "df_test = make_dataset(DATA_DIR, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d2d1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train_static['plavka_NAPR_ZAD'])\n",
    "df_train_static['plavka_NAPR_ZAD'] = le.transform(df_train_static['plavka_NAPR_ZAD'])\n",
    "df_test_static['plavka_NAPR_ZAD'] = le.transform(df_test_static['plavka_NAPR_ZAD'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(df_train_static['plavka_TIPE_FUR'])\n",
    "df_train_static['plavka_TIPE_FUR'] = le.transform(df_train_static['plavka_TIPE_FUR'])\n",
    "df_test_static['plavka_TIPE_FUR'] = le.transform(df_test_static['plavka_TIPE_FUR'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "plavka_nmz = set(df_train_static['plavka_NMZ'].unique().tolist()) & set(df_test_static['plavka_NMZ'].unique().tolist())\n",
    "df_train_static.loc[~df_train_static['plavka_NMZ'].isin(plavka_nmz), 'plavka_NMZ'] = 'None'\n",
    "df_test_static.loc[~df_test_static['plavka_NMZ'].isin(plavka_nmz), 'plavka_NMZ'] = 'None'\n",
    "le.fit(df_train_static['plavka_NMZ'])\n",
    "df_train_static['plavka_NMZ'] = le.transform(df_train_static['plavka_NMZ'])\n",
    "df_test_static['plavka_NMZ'] = le.transform(df_test_static['plavka_NMZ'])\n",
    "\n",
    "le = LabelEncoder()\n",
    "plavka_nmz = set(df_train_static['plavka_TIPE_GOL'].unique().tolist()) & set(df_test_static['plavka_TIPE_GOL'].unique().tolist())\n",
    "df_train_static.loc[~df_train_static['plavka_TIPE_GOL'].isin(plavka_nmz), 'plavka_TIPE_GOL'] = 'None'\n",
    "df_test_static.loc[~df_test_static['plavka_TIPE_GOL'].isin(plavka_nmz), 'plavka_TIPE_GOL'] = 'None'\n",
    "le.fit(df_train_static['plavka_TIPE_GOL'])\n",
    "df_train_static['plavka_TIPE_GOL'] = le.transform(df_train_static['plavka_TIPE_GOL'])\n",
    "df_test_static['plavka_TIPE_GOL'] = le.transform(df_test_static['plavka_TIPE_GOL'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(df_train_static[static_num_feats])\n",
    "df_train_static[static_num_feats] = ss.transform(df_train_static[static_num_feats])\n",
    "df_test_static[static_num_feats] = ss.transform(df_test_static[static_num_feats])\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "df_train_static[target_fts] = target_scaler.fit_transform(df_train_static[target_fts])\n",
    "\n",
    "le = LabelEncoder()\n",
    "plavka_nmz = set(df_train['NMSYP'].unique().tolist()) & set(df_test['NMSYP'].unique().tolist())\n",
    "df_train.loc[~df_train['NMSYP'].isin(plavka_nmz), 'NMSYP'] = 'None'\n",
    "df_test.loc[~df_test['NMSYP'].isin(plavka_nmz), 'NMSYP'] = 'None'\n",
    "le.fit(df_train['NMSYP'])\n",
    "df_train['NMSYP'] = le.transform(df_train['NMSYP'])\n",
    "df_test['NMSYP'] = le.transform(df_test['NMSYP'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(df_train[ts_num_feats])\n",
    "df_train[ts_num_feats] = ss.transform(df_train[ts_num_feats])\n",
    "df_test[ts_num_feats] = ss.transform(df_test[ts_num_feats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf5de99d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NPLV</th>\n",
       "      <th>Time</th>\n",
       "      <th>V</th>\n",
       "      <th>T</th>\n",
       "      <th>O2</th>\n",
       "      <th>N2</th>\n",
       "      <th>H2</th>\n",
       "      <th>CO2</th>\n",
       "      <th>CO</th>\n",
       "      <th>AR</th>\n",
       "      <th>...</th>\n",
       "      <th>T_ch</th>\n",
       "      <th>SI_ch</th>\n",
       "      <th>MN_ch</th>\n",
       "      <th>S_ch</th>\n",
       "      <th>P_ch</th>\n",
       "      <th>CR_ch</th>\n",
       "      <th>NI_ch</th>\n",
       "      <th>CU_ch</th>\n",
       "      <th>V_ch</th>\n",
       "      <th>TI_ch</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:11.437</td>\n",
       "      <td>-0.648392</td>\n",
       "      <td>-0.527127</td>\n",
       "      <td>0.937859</td>\n",
       "      <td>0.793358</td>\n",
       "      <td>-0.283170</td>\n",
       "      <td>-1.196558</td>\n",
       "      <td>-0.706850</td>\n",
       "      <td>0.420513</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.444865</td>\n",
       "      <td>-0.545556</td>\n",
       "      <td>-0.074593</td>\n",
       "      <td>1.713952</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>1.711605</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>0.803317</td>\n",
       "      <td>1.101082</td>\n",
       "      <td>-0.090980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:12.437</td>\n",
       "      <td>-0.648392</td>\n",
       "      <td>-0.527127</td>\n",
       "      <td>0.938872</td>\n",
       "      <td>0.793675</td>\n",
       "      <td>-0.282793</td>\n",
       "      <td>-1.198191</td>\n",
       "      <td>-0.706894</td>\n",
       "      <td>0.419008</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.444865</td>\n",
       "      <td>-0.545556</td>\n",
       "      <td>-0.074593</td>\n",
       "      <td>1.713952</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>1.711605</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>0.803317</td>\n",
       "      <td>1.101082</td>\n",
       "      <td>-0.090980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:13.437</td>\n",
       "      <td>-0.635787</td>\n",
       "      <td>-0.529519</td>\n",
       "      <td>0.939886</td>\n",
       "      <td>0.793991</td>\n",
       "      <td>-0.282417</td>\n",
       "      <td>-1.199824</td>\n",
       "      <td>-0.706938</td>\n",
       "      <td>0.417503</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.444865</td>\n",
       "      <td>-0.545556</td>\n",
       "      <td>-0.074593</td>\n",
       "      <td>1.713952</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>1.711605</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>0.803317</td>\n",
       "      <td>1.101082</td>\n",
       "      <td>-0.090980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:14.437</td>\n",
       "      <td>-0.623184</td>\n",
       "      <td>-0.530714</td>\n",
       "      <td>0.940900</td>\n",
       "      <td>0.794308</td>\n",
       "      <td>-0.282041</td>\n",
       "      <td>-1.201457</td>\n",
       "      <td>-0.706983</td>\n",
       "      <td>0.415998</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.444865</td>\n",
       "      <td>-0.545556</td>\n",
       "      <td>-0.074593</td>\n",
       "      <td>1.713952</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>1.711605</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>0.803317</td>\n",
       "      <td>1.101082</td>\n",
       "      <td>-0.090980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510008</td>\n",
       "      <td>2021-01-01 03:08:15.437</td>\n",
       "      <td>-0.635787</td>\n",
       "      <td>-0.534302</td>\n",
       "      <td>0.941913</td>\n",
       "      <td>0.794624</td>\n",
       "      <td>-0.281664</td>\n",
       "      <td>-1.203090</td>\n",
       "      <td>-0.707027</td>\n",
       "      <td>0.414493</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.444865</td>\n",
       "      <td>-0.545556</td>\n",
       "      <td>-0.074593</td>\n",
       "      <td>1.713952</td>\n",
       "      <td>0.842314</td>\n",
       "      <td>1.711605</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>0.803317</td>\n",
       "      <td>1.101082</td>\n",
       "      <td>-0.090980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315771</th>\n",
       "      <td>510125</td>\n",
       "      <td>2021-01-06 10:30:16.437</td>\n",
       "      <td>-0.825530</td>\n",
       "      <td>-0.951664</td>\n",
       "      <td>-0.185954</td>\n",
       "      <td>0.854826</td>\n",
       "      <td>-0.366402</td>\n",
       "      <td>-0.111855</td>\n",
       "      <td>-0.658951</td>\n",
       "      <td>1.168612</td>\n",
       "      <td>...</td>\n",
       "      <td>1.190704</td>\n",
       "      <td>1.508932</td>\n",
       "      <td>0.829609</td>\n",
       "      <td>0.353236</td>\n",
       "      <td>1.141905</td>\n",
       "      <td>0.170773</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>1.869034</td>\n",
       "      <td>0.716310</td>\n",
       "      <td>1.193907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315772</th>\n",
       "      <td>510125</td>\n",
       "      <td>2021-01-06 10:30:17.437</td>\n",
       "      <td>-0.800152</td>\n",
       "      <td>-0.951664</td>\n",
       "      <td>-0.183264</td>\n",
       "      <td>0.854686</td>\n",
       "      <td>-0.366326</td>\n",
       "      <td>-0.111855</td>\n",
       "      <td>-0.659067</td>\n",
       "      <td>1.168248</td>\n",
       "      <td>...</td>\n",
       "      <td>1.190704</td>\n",
       "      <td>1.508932</td>\n",
       "      <td>0.829609</td>\n",
       "      <td>0.353236</td>\n",
       "      <td>1.141905</td>\n",
       "      <td>0.170773</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>1.869034</td>\n",
       "      <td>0.716310</td>\n",
       "      <td>1.193907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315773</th>\n",
       "      <td>510125</td>\n",
       "      <td>2021-01-06 10:30:18.437</td>\n",
       "      <td>-0.800152</td>\n",
       "      <td>-0.951664</td>\n",
       "      <td>-0.180574</td>\n",
       "      <td>0.854546</td>\n",
       "      <td>-0.366249</td>\n",
       "      <td>-0.111855</td>\n",
       "      <td>-0.659183</td>\n",
       "      <td>1.167884</td>\n",
       "      <td>...</td>\n",
       "      <td>1.190704</td>\n",
       "      <td>1.508932</td>\n",
       "      <td>0.829609</td>\n",
       "      <td>0.353236</td>\n",
       "      <td>1.141905</td>\n",
       "      <td>0.170773</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>1.869034</td>\n",
       "      <td>0.716310</td>\n",
       "      <td>1.193907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315774</th>\n",
       "      <td>510125</td>\n",
       "      <td>2021-01-06 10:30:19.437</td>\n",
       "      <td>-0.762127</td>\n",
       "      <td>-0.951664</td>\n",
       "      <td>-0.177883</td>\n",
       "      <td>0.854406</td>\n",
       "      <td>-0.366172</td>\n",
       "      <td>-0.111855</td>\n",
       "      <td>-0.659298</td>\n",
       "      <td>1.167520</td>\n",
       "      <td>...</td>\n",
       "      <td>1.190704</td>\n",
       "      <td>1.508932</td>\n",
       "      <td>0.829609</td>\n",
       "      <td>0.353236</td>\n",
       "      <td>1.141905</td>\n",
       "      <td>0.170773</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>1.869034</td>\n",
       "      <td>0.716310</td>\n",
       "      <td>1.193907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315775</th>\n",
       "      <td>510125</td>\n",
       "      <td>2021-01-06 10:30:20.437</td>\n",
       "      <td>-0.838230</td>\n",
       "      <td>-0.951664</td>\n",
       "      <td>-0.175193</td>\n",
       "      <td>0.854266</td>\n",
       "      <td>-0.366095</td>\n",
       "      <td>-0.111855</td>\n",
       "      <td>-0.659414</td>\n",
       "      <td>1.167157</td>\n",
       "      <td>...</td>\n",
       "      <td>1.190704</td>\n",
       "      <td>1.508932</td>\n",
       "      <td>0.829609</td>\n",
       "      <td>0.353236</td>\n",
       "      <td>1.141905</td>\n",
       "      <td>0.170773</td>\n",
       "      <td>0.147078</td>\n",
       "      <td>1.869034</td>\n",
       "      <td>0.716310</td>\n",
       "      <td>1.193907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>315776 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          NPLV                    Time         V         T        O2  \\\n",
       "0       510008 2021-01-01 03:08:11.437 -0.648392 -0.527127  0.937859   \n",
       "1       510008 2021-01-01 03:08:12.437 -0.648392 -0.527127  0.938872   \n",
       "2       510008 2021-01-01 03:08:13.437 -0.635787 -0.529519  0.939886   \n",
       "3       510008 2021-01-01 03:08:14.437 -0.623184 -0.530714  0.940900   \n",
       "4       510008 2021-01-01 03:08:15.437 -0.635787 -0.534302  0.941913   \n",
       "...        ...                     ...       ...       ...       ...   \n",
       "315771  510125 2021-01-06 10:30:16.437 -0.825530 -0.951664 -0.185954   \n",
       "315772  510125 2021-01-06 10:30:17.437 -0.800152 -0.951664 -0.183264   \n",
       "315773  510125 2021-01-06 10:30:18.437 -0.800152 -0.951664 -0.180574   \n",
       "315774  510125 2021-01-06 10:30:19.437 -0.762127 -0.951664 -0.177883   \n",
       "315775  510125 2021-01-06 10:30:20.437 -0.838230 -0.951664 -0.175193   \n",
       "\n",
       "              N2        H2       CO2        CO        AR  ...      T_ch  \\\n",
       "0       0.793358 -0.283170 -1.196558 -0.706850  0.420513  ... -0.444865   \n",
       "1       0.793675 -0.282793 -1.198191 -0.706894  0.419008  ... -0.444865   \n",
       "2       0.793991 -0.282417 -1.199824 -0.706938  0.417503  ... -0.444865   \n",
       "3       0.794308 -0.282041 -1.201457 -0.706983  0.415998  ... -0.444865   \n",
       "4       0.794624 -0.281664 -1.203090 -0.707027  0.414493  ... -0.444865   \n",
       "...          ...       ...       ...       ...       ...  ...       ...   \n",
       "315771  0.854826 -0.366402 -0.111855 -0.658951  1.168612  ...  1.190704   \n",
       "315772  0.854686 -0.366326 -0.111855 -0.659067  1.168248  ...  1.190704   \n",
       "315773  0.854546 -0.366249 -0.111855 -0.659183  1.167884  ...  1.190704   \n",
       "315774  0.854406 -0.366172 -0.111855 -0.659298  1.167520  ...  1.190704   \n",
       "315775  0.854266 -0.366095 -0.111855 -0.659414  1.167157  ...  1.190704   \n",
       "\n",
       "           SI_ch     MN_ch      S_ch      P_ch     CR_ch     NI_ch     CU_ch  \\\n",
       "0      -0.545556 -0.074593  1.713952  0.842314  1.711605  0.147078  0.803317   \n",
       "1      -0.545556 -0.074593  1.713952  0.842314  1.711605  0.147078  0.803317   \n",
       "2      -0.545556 -0.074593  1.713952  0.842314  1.711605  0.147078  0.803317   \n",
       "3      -0.545556 -0.074593  1.713952  0.842314  1.711605  0.147078  0.803317   \n",
       "4      -0.545556 -0.074593  1.713952  0.842314  1.711605  0.147078  0.803317   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "315771  1.508932  0.829609  0.353236  1.141905  0.170773  0.147078  1.869034   \n",
       "315772  1.508932  0.829609  0.353236  1.141905  0.170773  0.147078  1.869034   \n",
       "315773  1.508932  0.829609  0.353236  1.141905  0.170773  0.147078  1.869034   \n",
       "315774  1.508932  0.829609  0.353236  1.141905  0.170773  0.147078  1.869034   \n",
       "315775  1.508932  0.829609  0.353236  1.141905  0.170773  0.147078  1.869034   \n",
       "\n",
       "            V_ch     TI_ch  \n",
       "0       1.101082 -0.090980  \n",
       "1       1.101082 -0.090980  \n",
       "2       1.101082 -0.090980  \n",
       "3       1.101082 -0.090980  \n",
       "4       1.101082 -0.090980  \n",
       "...          ...       ...  \n",
       "315771  0.716310  1.193907  \n",
       "315772  0.716310  1.193907  \n",
       "315773  0.716310  1.193907  \n",
       "315774  0.716310  1.193907  \n",
       "315775  0.716310  1.193907  \n",
       "\n",
       "[315776 rows x 29 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d1b6b201",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "class EvrazDatasetV(Dataset):\n",
    "    def __init__(self, feat_df, static_train_df):\n",
    "        self.feat_df = feat_df.copy()\n",
    "        self.static_train_df = static_train_df.copy()\n",
    "        \n",
    "        self.nlmv =  self.feat_df.NPLV.unique()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.nlmv)\n",
    "\n",
    "    def __getitem__(self, idx):    \n",
    "        nlmv = self.nlmv[idx]\n",
    "        return {\"static_cat\": self.static_train_df.loc[self.static_train_df.NPLV == nlmv, static_cat_feats].values[0],\n",
    "                \"static_numeric\": self.static_train_df.loc[self.static_train_df.NPLV == nlmv, static_num_feats].values[0],\n",
    "                \"ts_cat\": self.feat_df.loc[self.feat_df.NPLV == nlmv, ts_cat_feats].values,\n",
    "                \"ts_numeric\": self.feat_df.loc[self.feat_df.NPLV == nlmv, ts_num_feats].fillna(0).values,\n",
    "                \"y\": self.static_train_df.loc[self.static_train_df.NPLV == nlmv, target_fts].values[0]\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "133ae40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionWeightedAverage(nn.Module):\n",
    "    def __init__(self, hidden_dim: int, return_attention: bool = False):\n",
    "        super().__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.return_attention = return_attention\n",
    "        \n",
    "        self.attention_vector = nn.Parameter(\n",
    "            torch.empty(self.hidden_dim, dtype=torch.float32),\n",
    "            requires_grad=True,\n",
    "        )\n",
    "        nn.init.xavier_normal_(self.attention_vector.unsqueeze(-1))\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, mask: torch.Tensor\n",
    "    ):\n",
    "        logits = x.matmul(self.attention_vector)\n",
    "        ai = (logits - logits.max()).exp()\n",
    "\n",
    "        ai = ai * mask\n",
    "        att_weights = ai / (ai.sum(dim=1, keepdim=True) + 1e-12)\n",
    "        weighted_input = x * att_weights.unsqueeze(-1)\n",
    "        output = weighted_input.sum(dim=1)\n",
    "\n",
    "        if self.return_attention:\n",
    "            return output, att_weights\n",
    "        else:\n",
    "            return output, None\n",
    "        \n",
    "\n",
    "CONV_FEAT_SIZE = 8\n",
    "CONV_DELS = [1,2,4,8,16,32,64]\n",
    "\n",
    "class EvrazModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embs0 = nn.Embedding(3, 3)\n",
    "        self.embs1 = nn.Embedding(2, 3)\n",
    "        self.embs2 = nn.Embedding(4, 3)\n",
    "        self.embs3 = nn.Embedding(29, 3)\n",
    "        self.embs4 = nn.Embedding(4, 3)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                torch.nn.Conv1d(self.get_conv_size(i), CONV_FEAT_SIZE, 2, stride=1, \n",
    "                                padding=0, dilation=d, bias=False),\n",
    "                nn.BatchNorm1d(CONV_FEAT_SIZE),\n",
    "                nn.SiLU()\n",
    "            )\n",
    "            for i,d in enumerate(CONV_DELS)\n",
    "        ])\n",
    "        self.conv_attn = AttentionWeightedAverage(CONV_FEAT_SIZE)\n",
    "                \n",
    "        self.head = nn.Sequential(\n",
    "            nn.LayerNorm(CONV_FEAT_SIZE+len(static_num_feats)+3*4),\n",
    "            nn.Linear(CONV_FEAT_SIZE+len(static_num_feats)+3*4, 8),\n",
    "            nn.LayerNorm(8),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(8, 8),\n",
    "            nn.LayerNorm(8),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(8,2)\n",
    "        )\n",
    "        \n",
    "    def get_conv_size(self, i):\n",
    "        if i == 0:\n",
    "            return len(ts_num_feats)+3\n",
    "        return CONV_FEAT_SIZE\n",
    "    \n",
    "    def forward(self, static_cat, static_numeric, ts_cat, ts_numeric):\n",
    "        x0 = self.embs0(static_cat[:,0])\n",
    "        x1 = self.embs1(static_cat[:,1])\n",
    "        x2 = self.embs2(static_cat[:,2])\n",
    "        x3 = self.embs3(static_cat[:,3])\n",
    "        x4 = static_numeric\n",
    "        \n",
    "        y0 = self.embs4(ts_cat[:,:,0])\n",
    "        y = torch.cat([y0,ts_numeric], dim=-1).transpose(1,2)\n",
    "        \n",
    "        for m in self.convs:\n",
    "            y = m(y)\n",
    "        \n",
    "        y = y.transpose(1,2)\n",
    "        y = self.conv_attn(y, torch.ones((y.shape[0], y.shape[1])).to(y.device))[0]\n",
    "        \n",
    "        x = torch.cat([y,x0,x1,x2,x3,x4], dim=-1)\n",
    "        \n",
    "        return self.head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "1e02609d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EvrazModel().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d0c9bd1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = EvrazDatasetV(df_train, df_train_static)\n",
    "dl = DataLoader(ds, shuffle=False, batch_size=1, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "bf25ea54",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in dl:\n",
    "    out = model(x[\"static_cat\"].long().to(device),\n",
    "          x['static_numeric'].float().to(device),\n",
    "          x['ts_cat'].long().to(device),\n",
    "          x['ts_numeric'].float().to(device)\n",
    "         )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "54f89dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2516,  0.0375]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa70b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def metric(answers, user_csv):\n",
    "    delta_c = np.abs(np.array(answers[:,1]) - np.array(user_csv[:,1]))\n",
    "    hit_rate_c = np.int64(delta_c < 0.02)\n",
    "\n",
    "    delta_t = np.abs(np.array(answers[:,0]) - np.array(user_csv[:,0]))\n",
    "    hit_rate_t = np.int64(delta_t < 20)\n",
    "\n",
    "    N = np.size(answers[:,0])\n",
    "\n",
    "    return np.sum(hit_rate_c + hit_rate_t) / 2 / N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387f514d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=239)\n",
    "\n",
    "nplvs = df_train_static.NPLV.unique()\n",
    "for vipl_train, vipl_valid in kf.split(nplvs):\n",
    "    break\n",
    "nplv_train = [nplvs[x] for x in vipl_train]\n",
    "nplv_valid = [nplvs[x] for x in vipl_valid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2cddf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = EvrazDatasetV(df_train.loc[df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[df_train_staticNPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "train_dl = DataLoader(train_ds, shuffle=True, batch_size=1, num_workers=0)\n",
    "valid_ds = EvrazDatasetV(df_train.loc[~df_train.NPLV.isin(nplv_train)].reset_index(drop=True), \n",
    "                         df_train_static.loc[~df_train_staticNPLV.isin(nplv_train)].reset_index(drop=True))\n",
    "valid_dl = DataLoader(valid_ds, shuffle=False, batch_size=1, num_workers=0)\n",
    "\n",
    "criterion = nn.L1Loss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3, weight_decay=0.01)\n",
    "\n",
    "model.train();\n",
    "optimizer.zero_grad()\n",
    "\n",
    "best_metric = 0\n",
    "for epoch in range(100):\n",
    "    losses = []\n",
    "    for x in train_dl:\n",
    "        out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "             )\n",
    "        loss = criterion(out, x['y'].float().to(device))\n",
    "        losses.append(loss.item())\n",
    "        if len(losses) == 100:\n",
    "            print(torch.mean(torch.tensor(losses)))\n",
    "            losses = []\n",
    "        loss.backward()\n",
    "        optimizer.step() \n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    model.eval()\n",
    "    outs, ys = [], []\n",
    "    with torch.no_grad():\n",
    "        for x in valid_dl:\n",
    "            out = model(x[\"static_cat\"].long().to(device),\n",
    "              x['static_numeric'].float().to(device),\n",
    "              x['ts_cat'].long().to(device),\n",
    "              x['ts_numeric'].float().to(device)\n",
    "            )\n",
    "            ys.append(x['y'].numpy())\n",
    "            outs.append(out.detach().cpu().numpy())\n",
    "            \n",
    "    outs = np.concatenate(outs, axis=-1)\n",
    "    ys = np.concatenate(ys, axis=-1)\n",
    "    \n",
    "    outs = target_scaler.inverse_transform(outs)\n",
    "    ys = target_scaler.inverse_transform(ys)\n",
    "    \n",
    "    m = metric(outs, ys)\n",
    "    if m > best_metric:\n",
    "        torch.save(model.state_dict(), 'model0.pth')\n",
    "        best_metric = m\n",
    "    model.train()\n",
    "    optimizer.zero_grad()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
